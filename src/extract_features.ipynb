{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-luzzara/boardgame-complexity-predictor/blob/master/src/extract_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "gOE9-ZKtwDuH",
      "metadata": {
        "id": "gOE9-ZKtwDuH"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import os\n",
        "WORKING_LOCALLY = bool(os.getenv('WORKING_LOCALLY'))\n",
        "\n",
        "if WORKING_LOCALLY:\n",
        "    DATASET_FILE_PATH = 'data/dataset.csv'\n",
        "    CLEANED_DATASET_FILE_PATH = 'data/cleaned_dataset.csv'\n",
        "    PROCESSED_DATASET_FILE_PATH = 'data/processed_dataset.csv'\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATASET_FILE_PATH = '/content/drive/My Drive/Projects/IRBoardGameComplexity/dataset.csv'\n",
        "    CLEANED_DATASET_FILE_PATH = '/content/drive/My Drive/Projects/IRBoardGameComplexity/cleaned_dataset.csv'\n",
        "    PROCESSED_DATASET_FILE_PATH = '/content/drive/My Drive/Projects/IRBoardGameComplexity/processed_dataset.csv'\n",
        "    clear_output(wait=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3aa4a086-aa20-4def-b17a-3b4ff4fad93f",
      "metadata": {
        "id": "3aa4a086-aa20-4def-b17a-3b4ff4fad93f",
        "outputId": "f8980f2b-914e-4115-ad8a-b73341dbd33a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a4bcea26-c1a3-45fb-845e-98cae3073e85",
      "metadata": {
        "id": "a4bcea26-c1a3-45fb-845e-98cae3073e85"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger('bgg_predict')\n",
        "logger.handlers.clear()\n",
        "handler = logging.StreamHandler()\n",
        "formatter = logging.Formatter(\n",
        "        '%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "logger.addHandler(handler)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "logger.debug('test')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "def get_df_with_docs(file_path: str, nrows=None, skiprows=1) -> pd.DataFrame:\n",
        "    ''' get a dataframe containing nrows and skipping the first `skiprows` (including the header)'''\n",
        "    df_dataset = pd.read_csv(file_path, converters={ 'family': ast.literal_eval }, \n",
        "                             nrows=nrows, skiprows=range(1, skiprows))\n",
        "    return df_dataset\n",
        "\n",
        "def get_document_by_line(file_path: str, line: int) -> str:\n",
        "    ''' the line includes the header too '''\n",
        "    # range from 1 is used to keep the first row https://stackoverflow.com/a/27325729/5587393\n",
        "    df = get_df_with_docs(file_path, 1, line - 1)\n",
        "    return df['rulebook'].iloc[0]\n",
        "\n",
        "def get_document_by_id(file_path: str, id: int) -> str:\n",
        "     with pd.read_csv(file_path, chunksize=1, converters={ 'family': ast.literal_eval }) as reader:\n",
        "        while True:\n",
        "            df = next(reader)\n",
        "            bg_id = df['id'].iloc[0]\n",
        "            if bg_id == id:\n",
        "                return df['rulebook'].iloc[0]\n",
        "\n",
        "assert get_document_by_id(CLEANED_DATASET_FILE_PATH, 2310) == get_document_by_line(CLEANED_DATASET_FILE_PATH, 40)"
      ],
      "metadata": {
        "id": "VepMko8FPiyw"
      },
      "id": "VepMko8FPiyw",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning and Preprocessing\n",
        "\n",
        "In this part, data are cleaned and processed using coreference resolution. This means that all the pronouns and references to other objects in the sentence are resolved. The next 2 cells should be run only when you want to preprocess data, which takes a lot of time."
      ],
      "metadata": {
        "id": "Th5m4gIFheOC"
      },
      "id": "Th5m4gIFheOC"
    },
    {
      "cell_type": "code",
      "source": [
        "if not WORKING_LOCALLY:\n",
        "    !pip install spacy-transformers\n",
        "    !python3 -m pip install coreferee==1.3.*\n",
        "    !python3 -m coreferee install en\n",
        "    !python -m spacy download en_core_web_lg\n",
        "    !python -m spacy download en_core_web_trf\n",
        "    clear_output(wait=False)"
      ],
      "metadata": {
        "id": "G-Jyn4gUA6iw"
      },
      "id": "G-Jyn4gUA6iw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "regex_mail = re.compile(r'\\w+(?:\\.\\w+)*?@\\w+(?:\\.\\w+)+')\n",
        "# modified from https://stackoverflow.com/a/163684/5587393\n",
        "regex_link = re.compile(r'(?:\\b(?:(?:https?|ftp|file)://|www))[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#%=~_|]')\n",
        "# in a sentence there must be at least 4 words of length 2 each\n",
        "regex_at_least_4_words_in_sentence = re.compile(r\"^(?=.*?(?:[,:;()'\\\"]?[a-zA-Z']{2,}[,:;()'\\\"]?(?: |-|$)(?:[^a-zA-Z]*?|[a-zA-Z]? ?)){4,})\")         \n",
        "# a string like \"first.Second\" could be misinterpreted by the tokenizer as a single token\n",
        "# with the regex it becomes \"first. Second\"\n",
        "regex_distance_between_period_and_following_word = re.compile(r'\\.(?!\\s|$)')\n",
        "# compress consecutive whitespaces\n",
        "regex_multiple_spaces = re.compile(r'\\s{2,}')\n",
        "# interrupted words usually have a \"- \" at the end before the new line, 'inter- rupted' -> 'interrupted'\n",
        "# NOTE: must be after whitespace compression\n",
        "regex_interrupted_word = re.compile(r'([a-zA-Z])- ')\n",
        "# remove page numbers, that are usually enclosed in characters like = or -, for example \"-12-\"\n",
        "regex_consecutive_meaningless_chars = re.compile(r'[^\\.a-zA-Z0-9\\s()]{2,} *(?:\\d+)?|(?P<prepage>[^a-zA-Z\\s\\d\\.])\\d+(?P=prepage)')\n",
        "# remove paragraphs id, '1.2.3' -> ''\n",
        "regex_dot_separated_digits = re.compile(r'(?:\\d+\\.)+\\d+')\n",
        "# remove meaningless chars after sentence start, '. (- start' -> '. start'\n",
        "regex_clean_start = re.compile(r'\\.(\\s?)[^a-zA-Z\\s]+')\n",
        "# recover missing apices\n",
        "regex_missing_apices = re.compile(r\"\\b([a-zA-Z]+) (t|s)\\b\")\n",
        "\n",
        "def clean_from_short_sentences(text: str) -> str:\n",
        "    return '.'.join(sentence for sentence in text.split('.') if regex_at_least_4_words_in_sentence.match(sentence) is not None)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    for clean_function in [lambda x: regex_mail.sub('', x),\n",
        "                           lambda x: regex_link.sub('', x),\n",
        "                           lambda x: regex_dot_separated_digits.sub('', x),\n",
        "                           lambda x: regex_consecutive_meaningless_chars.sub('', x),\n",
        "                           lambda x: regex_clean_start.sub(r'.\\1', x),\n",
        "                           # everything that is remove should be placed before this line so that \n",
        "                           # eventual spaces are compressed with regex_multiple_space\n",
        "                           lambda x: regex_multiple_spaces.sub(' ', x),\n",
        "                           lambda x: regex_interrupted_word.sub(r'\\1', x),\n",
        "                           lambda x: regex_missing_apices.sub(r\"\\1'\\2\", x),\n",
        "                           lambda x: clean_from_short_sentences(x),\n",
        "                           lambda x: regex_distance_between_period_and_following_word.sub('. ', x)]:\n",
        "        text = clean_function(text)\n",
        "    return text\n",
        "\n",
        "test_text = 'this is a test (me@gmail.it) -12- that wi-  ll be   cleaned. with 2 5 6 not valid. two sentences can t be good http://or.not.'\n",
        "cleaned_text = clean_text(test_text)\n",
        "print(cleaned_text)\n",
        "assert cleaned_text == 'this is a test () that will be cleaned. two sentences can\\'t be good '"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBHkUGAsukWJ",
        "outputId": "8a30f746-a6c1-4c50-9f24-001debd626da",
        "collapsed": true
      },
      "id": "bBHkUGAsukWJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a test () that will be cleaned. two sentences can't be good \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import pandas as pd\n",
        "import coreferee\n",
        "\n",
        "def remove_columns_prefix(df: pd.DataFrame) -> None:\n",
        "    '''remove prefix 'info.' from the columns of df'''\n",
        "    df.rename(columns=lambda c: c.rsplit('.', 1)[-1], inplace=True)\n",
        "\n",
        "def _get_new_token_from_resolve(token: spacy.tokens.Token, \n",
        "                                chains: coreferee.data_model.ChainHolder) -> spacy.tokens.Token:\n",
        "    resolved_token = chains.resolve(token)\n",
        "    return token.text_with_ws if resolved_token is None \\\n",
        "                              else 'and '.join([res_token.text_with_ws + ' ' for res_token in resolved_token])   \n",
        "\n",
        "def _process_doc_for_coref(doc: spacy.tokens.Doc) -> str:\n",
        "    replacement_tokens = []\n",
        "    chains = doc._.coref_chains\n",
        "    new_doc_tokens_text = [_get_new_token_from_resolve(token, chains) for token in doc]\n",
        "\n",
        "    return ''.join(new_doc_tokens_text)\n",
        "\n",
        "def preprocess_texts(texts: List[str]) -> List[str]:\n",
        "    nlp = spacy.load('en_core_web_trf')\n",
        "    nlp.add_pipe(\"coreferee\")\n",
        "\n",
        "    texts = [clean_text(text) for text in texts]\n",
        "    docs = nlp.pipe(texts)\n",
        "\n",
        "    return [regex_multiple_spaces.sub(' ', _process_doc_for_coref(doc)) for doc in docs]\n",
        "        \n",
        "text = get_document_by_line(DATASET_FILE_PATH, 103)\n",
        "# text = '''Although he was very busy with his work, the magical Peter had had enough of it. \n",
        "#     He and his wife decided they needed a holiday. \n",
        "#     this couple travelled to Spain because it loves the country very much.'''\n",
        "preprocess_texts([text])"
      ],
      "metadata": {
        "id": "jpT4BvSfFWTd"
      },
      "id": "jpT4BvSfFWTd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import Generator, Tuple\n",
        "import ast\n",
        "\n",
        "START_CLEANING = False\n",
        "CHUNK_SIZE = 20\n",
        "DATASET_ROWS = sum(1 for line in open(DATASET_FILE_PATH))\n",
        "\n",
        "assert START_CLEANING == True # make sure you do not start preprocessing again\n",
        "\n",
        "def clean_data_row(row, docs_dict: Generator[Tuple[int, str], None, None]):\n",
        "    id_rulebook = next(docs_dict)\n",
        "    assert id_rulebook[0] == row['id']\n",
        "    row['rulebook'] = id_rulebook[1]\n",
        "    return row\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "for skip_rows in range(1, DATASET_ROWS, CHUNK_SIZE):\n",
        "    column_names = ['rulebook', 'info.id', 'info.name', 'info.averageweight', 'info.playingtime', 'info.family']\n",
        "    # ast.literal_eval converts the family column string into a python array\n",
        "    df_dataset = pd.read_csv(DATASET_FILE_PATH, converters={ 'info.family': ast.literal_eval },\n",
        "                            names=column_names, header=None,\n",
        "                            nrows=CHUNK_SIZE, skiprows=skip_rows)\n",
        "    remove_columns_prefix(df_dataset)\n",
        "    logger.info(f\"processing boardgames from {df_dataset.loc[0, 'id']} to {df_dataset.loc[df_dataset.index[-1], 'id']}\")\n",
        "    docs_dict = zip(df_dataset['id'].values, preprocess_texts(df_dataset['rulebook'].values))\n",
        "\n",
        "    df_cleaned_dataset = df_dataset.apply(lambda x: clean_data_row(x, docs_dict),\n",
        "                                        axis='columns')\n",
        "\n",
        "    df_cleaned_dataset.to_csv(CLEANED_DATASET_FILE_PATH, \n",
        "                            header=True if skip_rows == 1 else False, index=False, \n",
        "                            mode='w' if skip_rows == 1 else 'a')\n",
        "\n",
        "if not WORKING_LOCALLY:\n",
        "    drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "df_cleaned_dataset.head()"
      ],
      "metadata": {
        "id": "OfVY9Q2-TU02"
      },
      "id": "OfVY9Q2-TU02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Luck metrics\n",
        "these metrics are retrieved using rule-based matching and dependency matching. Luck is one of the criteria that determine the bg weight. In this case, the sources of luck considered are:\n",
        "\n",
        "- Dice rolling\n",
        "- Drawing\n",
        "- Shuffling\n",
        "- Words like *random* or *randomly*"
      ],
      "metadata": {
        "id": "KXWqWXyp3PVL"
      },
      "id": "KXWqWXyp3PVL"
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "from collections import namedtuple\n",
        "from spacy.matcher import Matcher, DependencyMatcher\n",
        "\n",
        "LuckMetrics = namedtuple('LuckMetrics', ['dice_based', 'drawing_based', 'shuffling_based', 'random_based'])\n",
        "\n",
        "def get_luck_metrics(doc: spacy.tokens.Doc) -> LuckMetrics:\n",
        "    # ---------- random ----------\n",
        "    random_matcher = Matcher(doc.vocab)\n",
        "    random_patterns_match = [\n",
        "        [{\"LEMMA\": { \"IN\": [\"random\", \"randomly\"]}}]\n",
        "    ]\n",
        "    random_matcher.add(\"random\", random_patterns_match)\n",
        "\n",
        "    # ---------- shuffle ----------\n",
        "    shuffle_matcher = Matcher(doc.vocab)\n",
        "    shuffle_patterns_match = [\n",
        "        [{\"LEMMA\": \"shuffle\"}]\n",
        "    ]\n",
        "    shuffle_matcher.add(\"shuffle\", shuffle_patterns_match)\n",
        "\n",
        "    # ---------- card drawing ----------\n",
        "    drawing_matcher = DependencyMatcher(doc.vocab)    \n",
        "    drawing_patterns = [\n",
        "        [\n",
        "            {\n",
        "                \"RIGHT_ID\": \"drawing\",\n",
        "                \"RIGHT_ATTRS\": {\"LEMMA\": \"draw\", \"POS\": \"VERB\"}\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"drawing\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"card\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": \"card\",\n",
        "                    \"POS\": \"NOUN\", \n",
        "                    \"DEP\": { \"IN\": ['dobj', 'nsubjpass', 'compound'] }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "    drawing_matcher.add(\"drawing\", drawing_patterns)\n",
        "    # ---------- dice rolling ----------\n",
        "    dice_matcher = DependencyMatcher(doc.vocab)    \n",
        "    dice_patterns = [\n",
        "        [\n",
        "            {\n",
        "                \"RIGHT_ID\": \"rolling\",\n",
        "                \"RIGHT_ATTRS\": {\"LEMMA\": { \"IN\": [\"use\", \"throw\", \"roll\"]}, \"POS\": \"VERB\"}\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"rolling\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"dice_or_die\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"die\", \"dice\"]},\n",
        "                    \"POS\": \"NOUN\", \n",
        "                    \"DEP\": { \"IN\": ['nsubj', 'dobj', 'nsubjpass', 'compound'] }\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"RIGHT_ID\": \"rolling\",\n",
        "                \"RIGHT_ATTRS\": {\"LEMMA\": { \"IN\": [\"use\", \"throw\", \"roll\"]}, \"POS\": \"VERB\"}\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"rolling\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"number\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"IS_DIGIT\": True, \n",
        "                    \"DEP\": { \"IN\": ['dobj'] }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "    dice_matcher.add(\"diceroll\", dice_patterns)\n",
        "\n",
        "    dice_matches = dice_matcher(doc) \n",
        "    draw_matches = drawing_matcher(doc)\n",
        "    shuffle_matches = shuffle_matcher(doc)\n",
        "    random_matches = random_matcher(doc)\n",
        "\n",
        "    return LuckMetrics(len(dice_matches), len(draw_matches), len(shuffle_matches), len(random_matches))\n",
        "\n",
        "text = get_document_by_line(CLEANED_DATASET_FILE_PATH, 130)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "print(len(doc), len(doc.text))\n",
        "print(get_luck_metrics(doc))"
      ],
      "metadata": {
        "id": "Ayy4X1vaYYCj",
        "outputId": "158dc3b0-92ae-494d-c7ee-c375934b5a85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ayy4X1vaYYCj",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2607 12624\n",
            "LuckMetrics(dice_based=12, drawing_based=4, shuffling_based=3, random_based=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Amount of choices metrics\n",
        "these metrics are retrieved using rule-based matching and dependency matching. The amount of choices every player has is one of the criteria that determine the bg weight. In this case, I am considering:\n",
        "\n",
        "- *can/could/may/decide/...*, with some exceptions:\n",
        "    - negatives are not considered choices, like *cannot draw* or *don't choose*\n",
        "    - *can* + *choose* and similar ones increase the *amount of choices* metrics by 1\n",
        "- *choice/option*, except when there is a leading *no*."
      ],
      "metadata": {
        "id": "vxB-jhmuFBrV"
      },
      "id": "vxB-jhmuFBrV"
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "from collections import namedtuple\n",
        "from spacy.matcher import Matcher, DependencyMatcher\n",
        "\n",
        "ChoiceMetrics = namedtuple('ChoiceMetrics', ['can_based', 'choice_based'])\n",
        "\n",
        "def get_choices_amount_metrics(doc: spacy.tokens.Doc) -> ChoiceMetrics:\n",
        "    # --------------  can/could/may -------------- \n",
        "    # all can/could/may\n",
        "    can_could_may_matcher = Matcher(doc.vocab)\n",
        "    can_could_may_patterns = [\n",
        "        [{\n",
        "            \"LEMMA\": { \"IN\": [\"can\", \"could\", \"may\"]}, \n",
        "            \"POS\": \"AUX\"\n",
        "        }]\n",
        "    ]\n",
        "    can_could_may_matcher.add('can_could_may', can_could_may_patterns)\n",
        "    can_could_may_matches = { match[1] for match in can_could_may_matcher(doc) }\n",
        "\n",
        "    # can/could/may with only or neg\n",
        "    can_could_may_exceptions_matcher = DependencyMatcher(doc.vocab)\n",
        "    can_could_may_exceptions_patterns = [\n",
        "        [\n",
        "            # ❌ can not/only/never verb \n",
        "            {\n",
        "                \"RIGHT_ID\": \"can_could_may\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"can\", \"could\", \"may\"]}, \n",
        "                    \"POS\": \"AUX\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"can_could_may\",\n",
        "                \"REL_OP\": \"<\",\n",
        "                \"RIGHT_ID\": \"generic_verb\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"POS\": { \"IN\": [\"AUX\", \"VERB\"] }\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"generic_verb\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"neg_or_only\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"not\", \"only\", \"never\"]}, \n",
        "                    \"DEP\": { \"IN\": [\"advmod\", \"neg\"] }\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            # ❌ can + choose are counted as 1\n",
        "            {\n",
        "                \"RIGHT_ID\": \"can_could_may\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"can\", \"could\", \"may\"]}, \n",
        "                    \"POS\": \"AUX\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"can_could_may\",\n",
        "                \"REL_OP\": \"<\",\n",
        "                \"RIGHT_ID\": \"decision_verb\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"decide\", \"select\", \"choose\", \"opt\"]},\n",
        "                    \"POS\": \"VERB\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "\n",
        "    can_could_may_exceptions_matcher.add('can_could_may_exceptions', can_could_may_exceptions_patterns)\n",
        "    can_could_may_exceptions_matches = { match[1][0] for match in can_could_may_exceptions_matcher(doc) }\n",
        "\n",
        "    # --------------  choose/select/... -------------- \n",
        "    # all choose/select/...\n",
        "    choose_matcher = Matcher(doc.vocab)\n",
        "    choose_patterns = [\n",
        "        [{\n",
        "            \"LEMMA\": { \"IN\": [\"decide\", \"select\", \"choose\", \"opt\"]}, \n",
        "            \"POS\": \"VERB\"\n",
        "        }]\n",
        "    ]\n",
        "    choose_matcher.add('choose_select', choose_patterns)\n",
        "    choose_matches = { match[1] for match in choose_matcher(doc) }\n",
        "\n",
        "    # choose/select/... with only or neg\n",
        "    choose_exceptions_matcher = DependencyMatcher(doc.vocab)\n",
        "    choose_exceptions_patterns = [\n",
        "        [\n",
        "            # ❌ not/only/never choose\n",
        "            {\n",
        "                \"RIGHT_ID\": \"decision_verb\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"decide\", \"select\", \"choose\", \"opt\"]}, \n",
        "                    \"POS\": \"VERB\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"decision_verb\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"negation\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"not\", \"only\", \"never\"]}, \n",
        "                    \"DEP\": { \"IN\": [\"advmod\", \"neg\"] }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "\n",
        "    choose_exceptions_matcher.add('choose_exceptions', choose_exceptions_patterns)\n",
        "    choose_exceptions_matches = { match[1][0] for match in choose_exceptions_matcher(doc) }\n",
        "\n",
        "    # -------------- choice and option -------------- \n",
        "    choice_option_matcher = Matcher(doc.vocab)\n",
        "    choice_option_patterns = [\n",
        "        [{\n",
        "            \"LEMMA\": { \"IN\": [\"choice\", \"option\"]}, \n",
        "            \"POS\": \"NOUN\"\n",
        "        }]\n",
        "    ]\n",
        "    choice_option_matcher.add('choice_option', choice_option_patterns)\n",
        "    choice_option_matches = { match[1] for match in choice_option_matcher(doc) }\n",
        "\n",
        "    choice_option_exceptions_matcher = DependencyMatcher(doc.vocab)\n",
        "    choice_option_exceptions_patterns = [\n",
        "        [\n",
        "            {\n",
        "                \"RIGHT_ID\": \"choice\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"choice\", \"option\"]}, \n",
        "                    \"POS\": \"NOUN\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"choice\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"prefix_no\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": \"no\",\n",
        "                    \"POS\": \"DET\",\n",
        "                    \"DEP\": \"det\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "    choice_option_exceptions_matcher.add('choice_option_exceptions', choice_option_exceptions_patterns)\n",
        "    choice_option_exceptions_matches = { match[1][0] for match in choice_option_exceptions_matcher(doc) }\n",
        "\n",
        "    # ---------------- results -----------------\n",
        "    can_based_len = len(can_could_may_matches.difference(can_could_may_exceptions_matches))\n",
        "    choice_based_len = len(choose_matches.difference(choose_exceptions_matches)) + \\\n",
        "                       len(choice_option_matches.difference(choice_option_exceptions_matches))\n",
        "\n",
        "    return ChoiceMetrics(can_based_len, choice_based_len)\n",
        "\n",
        "text = get_document_by_line(CLEANED_DATASET_FILE_PATH, 120)\n",
        "# text = '''you can only take this because it can be outrageous. \n",
        "#     you can't take it. you can not also choose. you can never be sure of the result. \n",
        "#     you can decide the next thing, or you choose the target. another choice is to win. \n",
        "#     but there is no right option.'''\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "print(len(doc), len(doc.text))\n",
        "print(get_choices_amount_metrics(doc))\n",
        "\n",
        "# displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "id": "vXvPodfk0ryj",
        "outputId": "83c75ff1-50f0-4291-e5f4-275d29c28331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vXvPodfk0ryj",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4276 19478\n",
            "ChoiceMetrics(can_based=22, choice_based=8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils\n",
        "Some useful methods, like:\n",
        "- `find_n_most_common_nouns()`: returns the most common tokens in the dataset\n",
        "- `displacy.render()`: shows the token dependencies"
      ],
      "metadata": {
        "id": "AdI0CNCkmhmQ"
      },
      "id": "AdI0CNCkmhmQ"
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''In a round of play, each player gets one turn.'''\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text.lower())\n",
        "print([(token.lemma_, token.head, token.dep_, token.pos_) for token in doc])\n",
        "print(len(doc), len(doc.text))\n",
        "\n",
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "id": "19Y_u25zdCnM",
        "outputId": "e30a4e7c-6b13-4a17-dc50-07f393ae8e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "id": "19Y_u25zdCnM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('in', gets, 'prep', 'ADP'), ('a', round, 'det', 'DET'), ('round', in, 'pobj', 'NOUN'), ('of', round, 'prep', 'ADP'), ('play', of, 'pobj', 'NOUN'), (',', gets, 'punct', 'PUNCT'), ('each', player, 'det', 'DET'), ('player', gets, 'nsubj', 'NOUN'), ('get', gets, 'ROOT', 'VERB'), ('one', turn, 'nummod', 'NUM'), ('turn', gets, 'dobj', 'NOUN'), ('.', gets, 'punct', 'PUNCT')]\n",
            "12 46\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7fa1cd9b19bc435a9ac20a955bad68a5-0\" class=\"displacy\" width=\"1800\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">round</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">of</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">play,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">each</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">player</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">gets</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">one</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">turn.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-2\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M395.0,266.5 L403.0,254.5 387.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-4\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-8\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,89.5 1620.0,89.5 1620.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-7fa1cd9b19bc435a9ac20a955bad68a5-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1620.0,266.5 L1628.0,254.5 1612.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Iterator\n",
        "\n",
        "def find_n_most_common_nouns(n, docs: Iterator[spacy.tokens.Doc]) -> List[Tuple[str, int]]:\n",
        "    docs_sets = [set(find_most_common_nouns(doc).keys())\n",
        "                 for doc in docs]\n",
        "    all_tokens_from_docs = itertools.chain(*docs_sets)\n",
        "    tokens_counter = Counter(all_tokens_from_docs)\n",
        "    return tokens_counter.most_common(n)\n",
        "    \n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "df_dataset = get_df_with_docs(CLEANED_DATASET_FILE_PATH, 100, 200)\n",
        "docs = nlp.pipe(df_dataset['rulebook'].values)\n",
        "\n",
        "find_n_most_common_nouns(80, docs)"
      ],
      "metadata": {
        "id": "w-JVFrfg4BwA"
      },
      "id": "w-JVFrfg4BwA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spacy Extension\n",
        "This extension makes the sentence number retrieval much faster. With this extension, after the parsing step of the pipeline, each token receives a `sentence_id` extension attribute. With `token.sent`, the sentences of the doc are visited every time until its position is within the target sentence boundaries."
      ],
      "metadata": {
        "id": "Afbz3YYhnIM-"
      },
      "id": "Afbz3YYhnIM-"
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.language import Language\n",
        "\n",
        "def _component_assign_sentence_id(doc: spacy.tokens.Doc) -> spacy.tokens.Doc:\n",
        "    spacy.tokens.Token.set_extension('sentence_id', default=None, force=True)\n",
        "    cur_sentence_id = -1\n",
        "    for token in doc:\n",
        "        if token.is_sent_start:\n",
        "            cur_sentence_id += 1\n",
        "        token._.sentence_id = cur_sentence_id\n",
        "        \n",
        "    return doc\n",
        "\n",
        "# this extension exists because accessing the `sent` attribute everytime means\n",
        "# loop through the doc.sents until the token is found\n",
        "factory_id = 'assign_sentence_id'\n",
        "if not Language.has_factory(factory_id):\n",
        "    @Language.component(factory_id)\n",
        "    def assign_sentence_id(doc: spacy.tokens.Doc) -> spacy.tokens.Doc:\n",
        "        return _component_assign_sentence_id(doc)\n",
        "\n",
        "text = \"\"\"this is the first sentence. this is the second one. the third one is here.\"\"\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('assign_sentence_id', after='parser')\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token, token._.sentence_id)"
      ],
      "metadata": {
        "id": "Uq749ZgDE1Ae",
        "outputId": "768ddf75-f3c5-4679-c428-038dddcbfb6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Uq749ZgDE1Ae",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this 0\n",
            "is 0\n",
            "the 0\n",
            "first 0\n",
            "sentence 0\n",
            ". 0\n",
            "this 1\n",
            "is 1\n",
            "the 1\n",
            "second 1\n",
            "one 1\n",
            ". 1\n",
            "the 2\n",
            "third 2\n",
            "one 2\n",
            "is 2\n",
            "here 2\n",
            ". 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rulebook features\n",
        "I search entities in the text using rules like the part of speech and frequency, instead of keyword extraction algorithm like `YAKE` or `TextRank`."
      ],
      "metadata": {
        "id": "YY-IHxn5nx7m"
      },
      "id": "YY-IHxn5nx7m"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "from typing import List, Set, Dict, Tuple\n",
        "from string import punctuation\n",
        "\n",
        "MAX_DISTANCE_TO_BE_CONSIDERED_UNIGRAM = 4\n",
        "MIN_TOKEN_TO_BE_CONSIDERED_UNIGRAM = 4\n",
        "MIN_TOKEN_TO_BE_CONSIDERED_BIGRAM = 3\n",
        "IGNORED_WORDS = {\n",
        "        'amount', 'beginning', 'board', 'book', 'bottom', 'case', 'choice', \n",
        "        'clarification', 'clockwise', 'condition', 'cost', 'design', 'difference', 'effect', \n",
        "        'end', 'example', 'face', 'front', 'game', 'left', 'middle', 'note', 'number', \n",
        "        'opponent', 'option', 'order', 'overview', 'page', 'play',\n",
        "        'purpose', 'reference', 'result', 'right', 'rule', 'rulebook', \n",
        "        'section', 'set', 'setup', 'side', 'summary', 'start', 'step', 'table', 'thing',\n",
        "        'type', 'tie', 'time', 'top', 'total', 'use', 'value', 'version', 'way'\n",
        "        }.union(spacy.load('en_core_web_sm').Defaults.stop_words)\n",
        "\n",
        "def find_most_common_nouns(doc: spacy.tokens.Doc) -> Dict[str, List[spacy.tokens.Token]]:\n",
        "    tokens_dict = defaultdict(list)\n",
        "\n",
        "    for token in doc:\n",
        "        if len(token) >= 3 and \\\n",
        "            token.pos_ in {'NOUN', 'PROPN'} and \\\n",
        "            token.dep_ in {'nsubj', 'dobj', 'nsubjpass', 'pobj'}:\n",
        "            tokens_dict[token.lemma_.lower()].append(token)\n",
        "           \n",
        "    return tokens_dict\n",
        "\n",
        "def _is_token_part_of_bigram(token: spacy.tokens.Token, \n",
        "                             unigram_token: spacy.tokens.Token) -> bool:\n",
        "    return token.dep_ == 'compound' and \\\n",
        "        token.pos_ in {'NOUN', 'PROPN'} and \\\n",
        "        not token.text.endswith(tuple(punctuation)) and \\\n",
        "        not token.text.startswith(tuple(punctuation)) and \\\n",
        "        token.head.i == unigram_token.i\n",
        "\n",
        "def find_most_relevant_ngram(doc: spacy.tokens.Doc,\n",
        "                             unigrams: Dict[str, List[spacy.tokens.Token]]) \\\n",
        "                             -> Dict[str, Set[str]]:\n",
        "    excluded_bigrams = IGNORED_WORDS.union(set(unigrams.keys()))\n",
        "    bigram_associated_dict = defaultdict(Counter)\n",
        "    for name, tokens in unigrams.items():\n",
        "        for token in tokens:\n",
        "            possible_bigram = doc[token.i - 1]\n",
        "            if token.i > 0 and _is_token_part_of_bigram(possible_bigram, token) and \\\n",
        "                possible_bigram.lemma_ not in excluded_bigrams:\n",
        "                bigram_associated_dict[name][possible_bigram.lemma_.lower()] += 1\n",
        "\n",
        "    return defaultdict(set, { \n",
        "        unigram: set(bigram for bigram, counter in bigrams.items() \n",
        "                     if counter >= MIN_TOKEN_TO_BE_CONSIDERED_BIGRAM)\n",
        "        for unigram, bigrams in bigram_associated_dict.items() \n",
        "    })\n",
        "\n",
        "def _is_token_an_unigram(token_info: Tuple[str, spacy.tokens.Token]) -> bool:\n",
        "    token = token_info[0]\n",
        "    occurrences = token_info[1]\n",
        "    sentence_ids = sorted([occ._.sentence_id for occ in occurrences])\n",
        "    return token not in IGNORED_WORDS and \\\n",
        "            len(occurrences) >= MIN_TOKEN_TO_BE_CONSIDERED_UNIGRAM and \\\n",
        "            any(token_occurrence.dep_ in {'nsubj', 'nsubjpass', 'dobj'} \\\n",
        "                for token_occurrence in occurrences) and \\\n",
        "            min( # get the minimum distance between sentence ids. A token must not be completely sparse \n",
        "                map(lambda x: x[1] - x[0], zip(sentence_ids[:-1], sentence_ids[1:]))\n",
        "            ) <= MAX_DISTANCE_TO_BE_CONSIDERED_UNIGRAM\n",
        "\n",
        "def find_most_relevant_unigrams(doc: spacy.tokens.Doc) \\\n",
        "                                -> Dict[str, List[spacy.tokens.Token]]:\n",
        "    possible_components_info = dict(\n",
        "        filter(lambda token_info: _is_token_an_unigram(token_info), \n",
        "               find_most_common_nouns(doc).items()))\n",
        "\n",
        "    return possible_components_info\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('assign_sentence_id', after='parser')\n",
        "text = get_document_by_line(CLEANED_DATASET_FILE_PATH, 190)\n",
        "doc = nlp(text.lower())\n",
        "unigrams = find_most_relevant_unigrams(doc)\n",
        "display(unigrams.keys())\n",
        "display(len(unigrams))\n",
        "ngrams = find_most_relevant_ngram(doc, unigrams)\n",
        "display(ngrams)"
      ],
      "metadata": {
        "id": "_t7h6TLUZoef",
        "outputId": "239b185b-88d9-44b2-bf37-09c620e0a68c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "id": "_t7h6TLUZoef",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dict_keys(['rounder', 'mountain', 'hill', 'trial', 'sprinter', 'rider', 'energy', 'peloton', 'movement', 'space', 'puncture', 'slipstream', 'team', 'sprint', 'cobblestone', 'race', 'ability', 'tpp', 'card'])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "defaultdict(set,\n",
              "            {'energy': {'turn'},\n",
              "             'team': {'racing'},\n",
              "             'race': set(),\n",
              "             'ability': set(),\n",
              "             'card': set()})"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the verbs associated to each entity. If an entity has many verbs \"associated\", it means the user can choose many ways to interact with it. "
      ],
      "metadata": {
        "id": "7dyKIgYsiFAr"
      },
      "id": "7dyKIgYsiFAr"
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import DependencyMatcher\n",
        "from typing import Dict\n",
        "\n",
        "def find_actions_count_for_unigrams(doc: spacy.tokens.Doc, \n",
        "                                    unigrams: Dict[str, List[spacy.tokens.Token]]) -> Dict[str, int]:\n",
        "    return { unigram: len(set(token.head.lemma_ for token in unigrams[unigram] if token.head.pos_ == 'VERB'))\n",
        "        for unigram in unigrams }\n",
        "\n",
        "def get_actions_score(doc: spacy.tokens.Doc, \n",
        "                      unigrams: Dict[str, List[spacy.tokens.Token]]) -> float:\n",
        "    actions_counts = find_actions_count_for_unigrams(doc, unigrams)\n",
        "    return sum(unigram_action_count[1] for unigram_action_count in actions_counts.items()) / len(unigrams)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('assign_sentence_id', after='parser')\n",
        "text = get_document_by_line(CLEANED_DATASET_FILE_PATH, 191)\n",
        "doc = nlp(text.lower())\n",
        "unigrams = find_most_relevant_unigrams(doc)\n",
        "actions_counts = find_actions_count_for_unigrams(doc, unigrams)\n",
        "print(actions_counts)\n",
        "get_actions_score(doc, unigrams)"
      ],
      "metadata": {
        "id": "I7tz7cETg5qU",
        "outputId": "e01fca78-113d-4bc8-999f-ace570999078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "I7tz7cETg5qU",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'scenario': 4, 'mansion': 4, 'card': 35, 'change': 4, 'action': 17, 'keeper': 36, 'threat': 8, 'player': 14, 'deck': 1, 'event': 3, 'investigator': 45, 'objective': 3, 'point': 7, 'item': 4, 'death': 2, 'monster': 12, 'room': 15, 'fire': 6, 'darkness': 7, 'zombie': 9, 'damage': 6, 'token': 6, 'space': 8, 'figure': 8, 'turn': 7, 'stun': 2, 'attribute': 2, 'boy': 17, 'marker': 3, 'spell': 2, 'requirement': 2, 'horror': 9, 'place': 5, 'sample': 4, 'strike': 1, 'lynch': 5, 'man': 6, 'door': 5, 'weapon': 10, 'test': 5, 'puzzle': 3, 'monastery': 1, 'voice': 7, 'head': 1, 'cultist': 8, 'leader': 2, 'abomination': 9, 'relative': 3, 'altar': 1, 'ground': 2, 'tile': 3, 'classroom': 1, 'light': 4, 'witch': 6, 'health': 3, 'creature': 2, 'hound': 4, 'jacket': 3, 'helena': 2, 'skull': 4, 'police': 3, 'world': 2, 'sanctum': 3, 'sanity': 3, 'line': 1, 'girl': 3, 'injury': 3}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.507462686567164"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "from typing import List, Set, Dict\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "RulebookMetrics = namedtuple(\"RulebookMetrics\", ['entities_count', \n",
        "                                                 'interaction_score', \n",
        "                                                 'entities_variance',\n",
        "                                                 'actions_score'])\n",
        "\n",
        "def get_interaction_count(unigrams: Dict[str, List[spacy.tokens.Token]]) -> float:\n",
        "    matrix_len = len(unigrams)\n",
        "    matrix = [[0 for c in range(matrix_len)] for r in range(matrix_len)]\n",
        "    unigrams_product = product(enumerate(unigrams.items()), enumerate(unigrams.items()))\n",
        "    for (ir, (ug1, tokens1)), (ic, (ug2, tokens2)) in unigrams_product:\n",
        "        # I only fill half of the matrix, the other half is symmetrical to the first\n",
        "        # one. The main diagonal is useless because there is no interaction between\n",
        "        # a component and itself, by definition\n",
        "        if ir < ic:\n",
        "            matrix[ir][ic] = len(set(token._.sentence_id for token in tokens1) \\\n",
        "                .union(set(token._.sentence_id for token in tokens2)))\n",
        "        \n",
        "    logger.debug(matrix)\n",
        "\n",
        "    # the graph density of an undirected graph consider an edge as 2\n",
        "    return 2 * sum(sum(_ for _ in row) for row in matrix)\n",
        "\n",
        "# TODO: is it useful?\n",
        "# def get_relevant_sentences_count(unigrams: Dict[str, List[spacy.tokens.Token]]) -> int:\n",
        "#     return len(set().union(\n",
        "#         *[set(token._.sentence_id for token in tokens) \n",
        "#           for tokens in unigrams.values()]))\n",
        "\n",
        "def get_entities_variance(doc: spacy.tokens.Doc, \n",
        "                          unigrams: Dict[str, List[spacy.tokens.Token]]) -> float:\n",
        "    '''variance measures how components interleave in the text. This could mean that rules involve\n",
        "    many components and are therefore more complex. variancy is computed using `np.var` on each\n",
        "    component list. the results are normalized by multiplicating for the frequency of the component.\n",
        "    eventually the partial variances are summed together and the result normalized with the \n",
        "    total numbers of sentences.'''\n",
        "    tokens_count = sum(len(token_list) for token_list in unigrams.values())\n",
        "    return sum((len(tokens) / tokens_count) * np.var([token._.sentence_id for token in tokens])\n",
        "        for tokens in unigrams.values()) / (doc[-1]._.sentence_id + 1)\n",
        "\n",
        "def get_rulebook_metrics(doc: spacy.tokens.Doc) -> RulebookMetrics:\n",
        "    most_relevant_unigrams = find_most_relevant_unigrams(doc)\n",
        "    logger.debug(most_relevant_unigrams)\n",
        "    most_relevant_ngrams = find_most_relevant_ngram(doc, most_relevant_unigrams)\n",
        "    interactions_count = get_interaction_count(most_relevant_unigrams)\n",
        "    # entities includes unigrams + their ngrams. `max(0, len(ngrams) - 1)` because\n",
        "    # if I have 4 types of bigrams, then I need to add 3, given that 1 is included\n",
        "    # in the unigrams\n",
        "    most_relevant_entities_count = len(most_relevant_unigrams) + \\\n",
        "        sum(max(0, len(ngrams) - 1) for ngrams in most_relevant_ngrams.values())\n",
        "    entities_variance = get_entities_variance(doc, most_relevant_unigrams)\n",
        "    # density of a network\n",
        "    interactions_score = interactions_count / (most_relevant_entities_count * (most_relevant_entities_count - 1))\n",
        "    actions_score = get_actions_score(doc, most_relevant_unigrams)\n",
        "\n",
        "    return RulebookMetrics(most_relevant_entities_count, \n",
        "                           interactions_score,\n",
        "                           entities_variance,\n",
        "                           actions_score)\n",
        "\n",
        "text = get_document_by_line(CLEANED_DATASET_FILE_PATH, 139)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('assign_sentence_id', after='parser')\n",
        "doc = nlp(text.lower())\n",
        "components = get_rulebook_metrics(doc)\n",
        "components"
      ],
      "metadata": {
        "id": "TKh5Jf-xovbn",
        "outputId": "34c99660-607a-4f5d-c9d2-a8630b9c9e49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TKh5Jf-xovbn",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RulebookMetrics(entities_count=21, interaction_score=28.36190476190476, entities_variance=10.731811818779741, actions_score=6.55)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing step"
      ],
      "metadata": {
        "id": "ZqrhRHBToT6q"
      },
      "id": "ZqrhRHBToT6q"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7d6596cd-3fd1-4e50-9dce-e6c333667182",
      "metadata": {
        "id": "7d6596cd-3fd1-4e50-9dce-e6c333667182",
        "outputId": "1f7a34e3-793c-46e4-8c42-4ef91913abd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-04 11:12:09,775 bgg_predict  INFO     processing boardgames from 10 to 590\n",
            "INFO:bgg_predict:processing boardgames from 10 to 590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-04 11:12:20,290 bgg_predict  INFO     processing boardgames from 690 to 2350\n",
            "INFO:bgg_predict:processing boardgames from 690 to 2350\n",
            "2023-01-04 11:12:51,592 bgg_predict  INFO     processing boardgames from 2780 to 6830\n",
            "INFO:bgg_predict:processing boardgames from 2780 to 6830\n",
            "2023-01-04 11:13:17,276 bgg_predict  INFO     processing boardgames from 6860 to 9870\n",
            "INFO:bgg_predict:processing boardgames from 6860 to 9870\n",
            "2023-01-04 11:13:37,239 bgg_predict  INFO     processing boardgames from 10140 to 17240\n",
            "INFO:bgg_predict:processing boardgames from 10140 to 17240\n",
            "2023-01-04 11:13:59,815 bgg_predict  INFO     processing boardgames from 17970 to 24310\n",
            "INFO:bgg_predict:processing boardgames from 17970 to 24310\n",
            "2023-01-04 11:14:16,081 bgg_predict  INFO     processing boardgames from 24480 to 33950\n",
            "INFO:bgg_predict:processing boardgames from 24480 to 33950\n",
            "2023-01-04 11:14:50,456 bgg_predict  INFO     processing boardgames from 34010 to 43530\n",
            "INFO:bgg_predict:processing boardgames from 34010 to 43530\n",
            "2023-01-04 11:15:11,549 bgg_predict  INFO     processing boardgames from 43570 to 69120\n",
            "INFO:bgg_predict:processing boardgames from 43570 to 69120\n",
            "2023-01-04 11:15:27,554 bgg_predict  INFO     processing boardgames from 69130 to 91080\n",
            "INFO:bgg_predict:processing boardgames from 69130 to 91080\n",
            "2023-01-04 11:15:46,589 bgg_predict  INFO     processing boardgames from 91620 to 112210\n",
            "INFO:bgg_predict:processing boardgames from 91620 to 112210\n",
            "2023-01-04 11:16:05,871 bgg_predict  INFO     processing boardgames from 114530 to 126000\n",
            "INFO:bgg_predict:processing boardgames from 114530 to 126000\n",
            "2023-01-04 11:16:18,135 bgg_predict  INFO     processing boardgames from 126750 to 147030\n",
            "INFO:bgg_predict:processing boardgames from 126750 to 147030\n",
            "2023-01-04 11:16:43,313 bgg_predict  INFO     processing boardgames from 147170 to 160010\n",
            "INFO:bgg_predict:processing boardgames from 147170 to 160010\n",
            "2023-01-04 11:16:59,883 bgg_predict  INFO     processing boardgames from 161530 to 174570\n",
            "INFO:bgg_predict:processing boardgames from 161530 to 174570\n",
            "2023-01-04 11:17:11,474 bgg_predict  INFO     processing boardgames from 174660 to 188390\n",
            "INFO:bgg_predict:processing boardgames from 174660 to 188390\n",
            "2023-01-04 11:17:33,456 bgg_predict  INFO     processing boardgames from 190400 to 203430\n",
            "INFO:bgg_predict:processing boardgames from 190400 to 203430\n",
            "2023-01-04 11:17:51,447 bgg_predict  INFO     processing boardgames from 203780 to 220520\n",
            "INFO:bgg_predict:processing boardgames from 203780 to 220520\n",
            "2023-01-04 11:18:13,595 bgg_predict  INFO     processing boardgames from 220780 to 233020\n",
            "INFO:bgg_predict:processing boardgames from 220780 to 233020\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "380"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   averageweight  playingtime  rulebook_len  dice_luck_metric  \\\n",
              "0         2.1579           60        3155.0               0.0   \n",
              "1         3.1452           90       73093.0               0.0   \n",
              "2         1.8100           90       20155.0               0.0   \n",
              "3         1.4858           30       13866.0               0.0   \n",
              "4         2.7813          120        3604.0               0.0   \n",
              "\n",
              "   drawing_luck_metric  shuffling_luck_metric  random_luck_metric  can_metric  \\\n",
              "0             0.000000               0.147710            0.000000    0.590842   \n",
              "1             0.013557               0.027113            0.027113    1.342100   \n",
              "2             0.000000               0.000000            0.000000    0.911927   \n",
              "3             0.413936               0.137979            0.000000    0.551914   \n",
              "4             0.000000               0.000000            0.000000    0.583090   \n",
              "\n",
              "   choices_metric  entities_count  ...  actions_score  abstracts  cgs  \\\n",
              "0        0.295421             5.0  ...       8.000000          0    0   \n",
              "1        0.135566            62.0  ...       6.150000          0    0   \n",
              "2        0.143988            39.0  ...       5.769231          0    0   \n",
              "3        0.275957            14.0  ...       4.000000          0    0   \n",
              "4        1.020408             8.0  ...       4.000000          0    0   \n",
              "\n",
              "   childrensgames  familygames  partygames  strategygames  thematic  \\\n",
              "0               0            1           0              0         0   \n",
              "1               0            0           0              1         0   \n",
              "2               0            0           0              0         1   \n",
              "3               0            1           0              0         0   \n",
              "4               0            0           0              1         0   \n",
              "\n",
              "   unspecified  wargames  \n",
              "0            0         0  \n",
              "1            0         0  \n",
              "2            0         0  \n",
              "3            0         0  \n",
              "4            0         0  \n",
              "\n",
              "[5 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3317cff9-9b56-4a96-968f-162cc614e9cc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>averageweight</th>\n",
              "      <th>playingtime</th>\n",
              "      <th>rulebook_len</th>\n",
              "      <th>dice_luck_metric</th>\n",
              "      <th>drawing_luck_metric</th>\n",
              "      <th>shuffling_luck_metric</th>\n",
              "      <th>random_luck_metric</th>\n",
              "      <th>can_metric</th>\n",
              "      <th>choices_metric</th>\n",
              "      <th>entities_count</th>\n",
              "      <th>...</th>\n",
              "      <th>actions_score</th>\n",
              "      <th>abstracts</th>\n",
              "      <th>cgs</th>\n",
              "      <th>childrensgames</th>\n",
              "      <th>familygames</th>\n",
              "      <th>partygames</th>\n",
              "      <th>strategygames</th>\n",
              "      <th>thematic</th>\n",
              "      <th>unspecified</th>\n",
              "      <th>wargames</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.1579</td>\n",
              "      <td>60</td>\n",
              "      <td>3155.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.147710</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.590842</td>\n",
              "      <td>0.295421</td>\n",
              "      <td>5.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.1452</td>\n",
              "      <td>90</td>\n",
              "      <td>73093.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013557</td>\n",
              "      <td>0.027113</td>\n",
              "      <td>0.027113</td>\n",
              "      <td>1.342100</td>\n",
              "      <td>0.135566</td>\n",
              "      <td>62.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6.150000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.8100</td>\n",
              "      <td>90</td>\n",
              "      <td>20155.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.911927</td>\n",
              "      <td>0.143988</td>\n",
              "      <td>39.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.769231</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.4858</td>\n",
              "      <td>30</td>\n",
              "      <td>13866.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.413936</td>\n",
              "      <td>0.137979</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.551914</td>\n",
              "      <td>0.275957</td>\n",
              "      <td>14.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.7813</td>\n",
              "      <td>120</td>\n",
              "      <td>3604.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.583090</td>\n",
              "      <td>1.020408</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3317cff9-9b56-4a96-968f-162cc614e9cc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3317cff9-9b56-4a96-968f-162cc614e9cc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3317cff9-9b56-4a96-968f-162cc614e9cc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from typing import Tuple\n",
        "import pandas as pd\n",
        "import ast\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "from collections import namedtuple\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "CHUNK_SIZE = 20\n",
        "DATASET_ROWS = sum(1 for line in open(CLEANED_DATASET_FILE_PATH))\n",
        "RulebookFeatures = namedtuple('RulebookFeatures', ['rulebook_len',\n",
        "                                                   'dice_luck_metric', \n",
        "                                                   'drawing_luck_metric', \n",
        "                                                   'shuffling_luck_metric', \n",
        "                                                   'random_luck_metric', \n",
        "                                                   'can_metric',\n",
        "                                                   'choices_metric',\n",
        "                                                   'entities_count',\n",
        "                                                   'interaction_score',\n",
        "                                                   'entities_variance',\n",
        "                                                   'actions_score'])\n",
        "\n",
        "def get_rules_features(doc: spacy.tokens.Doc) -> RulebookFeatures:\n",
        "    normalization_factor = len(doc) / 100\n",
        "    rulebook_len = len(doc.text)\n",
        "    luck_metrics = get_luck_metrics(doc)\n",
        "    choices_amount_metric = get_choices_amount_metrics(doc)\n",
        "    rulebook_metrics = get_rulebook_metrics(doc)\n",
        "\n",
        "    return RulebookFeatures(rulebook_len=rulebook_len,\n",
        "                            dice_luck_metric=luck_metrics.dice_based / normalization_factor,\n",
        "                            drawing_luck_metric=luck_metrics.drawing_based / normalization_factor,\n",
        "                            shuffling_luck_metric=luck_metrics.shuffling_based / normalization_factor,\n",
        "                            random_luck_metric=luck_metrics.random_based / normalization_factor,\n",
        "                            can_metric=choices_amount_metric.can_based / normalization_factor,\n",
        "                            choices_metric=choices_amount_metric.choice_based / normalization_factor,\n",
        "                            entities_count=rulebook_metrics.entities_count,\n",
        "                            interaction_score=rulebook_metrics.interaction_score,\n",
        "                            entities_variance=rulebook_metrics.entities_variance,\n",
        "                            actions_score=rulebook_metrics.actions_score)\n",
        "\n",
        "def process_texts(texts: List[str]) -> List[RulebookFeatures]:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    nlp.add_pipe('assign_sentence_id', after='parser')\n",
        "    texts = [text.lower() for text in texts]\n",
        "    docs = nlp.pipe(texts)\n",
        "\n",
        "    return docs\n",
        "\n",
        "def create_feature_series(row, docs_dict):\n",
        "    next_doc_info = next(docs_dict)\n",
        "    assert next_doc_info[0] == row.id\n",
        "    return pd.Series(get_rules_features(next_doc_info[1]), \n",
        "                     index=RulebookFeatures._fields)\n",
        "    \n",
        "def family_field_eval(family: str):\n",
        "    result = ast.literal_eval(family)\n",
        "    return result if len(result) > 0 else ['unspecified']\n",
        "\n",
        "df_features = pd.DataFrame()\n",
        "for skip_rows in range(1, DATASET_ROWS, CHUNK_SIZE):\n",
        "    column_names = ['rulebook', 'id', 'name', 'averageweight', 'playingtime', 'family']\n",
        "    # ast.literal_eval converts the family column string into a python array\n",
        "    df_dataset = pd.read_csv(CLEANED_DATASET_FILE_PATH, converters={ 'family': family_field_eval },\n",
        "                            names=column_names, header=None,\n",
        "                            nrows=CHUNK_SIZE, skiprows=skip_rows)\n",
        "    logger.info(f\"processing boardgames from {df_dataset.loc[0, 'id']} to {df_dataset.loc[df_dataset.index[-1], 'id']}\")\n",
        "    docs_dict = zip(df_dataset['id'].values, process_texts(df_dataset['rulebook'].values))\n",
        "    df_rulebook_features = df_dataset.apply(lambda x: create_feature_series(x, docs_dict),\n",
        "                                            axis='columns')\n",
        "\n",
        "    df_features = pd.concat([df_features, df_dataset[['averageweight', 'playingtime', 'family']] \\\n",
        "                    .join(df_rulebook_features)])\n",
        "    \n",
        "display(len(df_features))\n",
        "\n",
        "# one-hot encoding \"family\" field \n",
        "# from https://stackoverflow.com/questions/71401193/one-hot-encoding-in-python-for-array-values-in-a-dataframe\n",
        "df_features = pd.concat([df_features, \n",
        "                         df_features.pop('family')\n",
        "                                    .apply('|'.join)\n",
        "                                    .str.get_dummies()], axis='columns')\n",
        "\n",
        "df_features.to_csv(PROCESSED_DATASET_FILE_PATH, header=True, index=False, mode='w')    \n",
        "if not WORKING_LOCALLY:\n",
        "    drive.flush_and_unmount()\n",
        "\n",
        "df_features.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}