{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-luzzara/boardgame-complexity-predictor/blob/master/src/extract_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "gOE9-ZKtwDuH",
      "metadata": {
        "id": "gOE9-ZKtwDuH"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import os\n",
        "WORKING_LOCALLY = bool(os.getenv('WORKING_LOCALLY'))\n",
        "\n",
        "if WORKING_LOCALLY:\n",
        "    DATASET_FILE_PATH = 'data/dataset.csv'\n",
        "    CLEANED_DATASET_FILE_PATH = 'data/cleaned_dataset.csv'\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATASET_FILE_PATH = '/content/drive/My Drive/Projects/IRBoardGameComplexity/dataset.csv'\n",
        "    CLEANED_DATASET_FILE_PATH = '/content/drive/My Drive/Projects/IRBoardGameComplexity/cleaned_dataset.csv'\n",
        "    # !pip install git+https://github.com/LIAAD/yake\n",
        "    # !pip install rake-nltk\n",
        "    clear_output(wait=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3aa4a086-aa20-4def-b17a-3b4ff4fad93f",
      "metadata": {
        "id": "3aa4a086-aa20-4def-b17a-3b4ff4fad93f"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import spacy\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a4bcea26-c1a3-45fb-845e-98cae3073e85",
      "metadata": {
        "id": "a4bcea26-c1a3-45fb-845e-98cae3073e85",
        "outputId": "dfd3bcb0-092b-4b77-8c4f-4b6a9bb75e2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-11-29 20:07:52,027 bgg_predict  DEBUG    test\n",
            "DEBUG:bgg_predict:test\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger('bgg_predict')\n",
        "logger.handlers.clear()\n",
        "handler = logging.StreamHandler()\n",
        "formatter = logging.Formatter(\n",
        "        '%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "logger.addHandler(handler)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "logger.debug('test')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "def get_df_with_docs(file_path: str, nrows=None, skiprows=1) -> pd.DataFrame:\n",
        "    ''' get a dataframe containing nrows and skipping the first `skiprows` (including the header)'''\n",
        "    df_dataset = pd.read_csv(file_path, converters={ 'family': ast.literal_eval }, \n",
        "                             nrows=nrows, skiprows=range(1, skiprows))\n",
        "    return df_dataset\n",
        "\n",
        "def get_document_by_line(file_path: str, line: int) -> str:\n",
        "    ''' the line includes the header too '''\n",
        "    # range from 1 is used to keep the first row https://stackoverflow.com/a/27325729/5587393\n",
        "    df = get_df_with_docs(file_path, 1, line - 1)\n",
        "    return df['rulebook'].iloc[0]\n",
        "\n",
        "def get_document_by_id(file_path: str, id: int) -> str:\n",
        "     with pd.read_csv(file_path, chunksize=1, converters={ 'family': ast.literal_eval }) as reader:\n",
        "        while True:\n",
        "            df = next(reader)\n",
        "            bg_id = df['id'].iloc[0]\n",
        "            if bg_id == id:\n",
        "                return df['rulebook'].iloc[0]\n",
        "\n",
        "assert get_document_by_id(CLEANED_DATASET_FILE_PATH, 2310) == get_document_by_line(CLEANED_DATASET_FILE_PATH, 40)"
      ],
      "metadata": {
        "id": "VepMko8FPiyw"
      },
      "id": "VepMko8FPiyw",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning and Preprocessing\n",
        "\n",
        "In this part, data are cleaned and processed using coreference resolution. This means that all the pronouns and references to other objects in the sentence are resolved. The next 2 cells should be run only when you want to preprocess data, which takes a lot of time."
      ],
      "metadata": {
        "id": "Th5m4gIFheOC"
      },
      "id": "Th5m4gIFheOC"
    },
    {
      "cell_type": "code",
      "source": [
        "if not WORKING_LOCALLY:\n",
        "    !pip install spacy-transformers\n",
        "    !python3 -m pip install coreferee==1.3.*\n",
        "    !python3 -m coreferee install en\n",
        "    !python -m spacy download en_core_web_lg\n",
        "    !python -m spacy download en_core_web_trf\n",
        "    clear_output(wait=False)"
      ],
      "metadata": {
        "id": "G-Jyn4gUA6iw"
      },
      "id": "G-Jyn4gUA6iw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "regex_mail = re.compile(r'\\w+(?:\\.\\w+)*?@\\w+(?:\\.\\w+)+')\n",
        "# modified from https://stackoverflow.com/a/163684/5587393\n",
        "regex_link = re.compile(r'(?:\\b(?:(?:https?|ftp|file)://|www))[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#%=~_|]')\n",
        "# in a sentence there must be at least 4 words of length 2 each\n",
        "regex_at_least_4_words_in_sentence = re.compile(r\"^(?=.*?(?:[,:;()'\\\"]?[a-zA-Z']{2,}[,:;()'\\\"]?(?: |-|$)(?:[^a-zA-Z]*?|[a-zA-Z]? ?)){4,})\")         \n",
        "# a string like \"first.Second\" could be misinterpreted by the tokenizer as a single token\n",
        "# with the regex it becomes \"first. Second\"\n",
        "regex_distance_between_period_and_following_word = re.compile(r'\\.(?!\\s|$)')\n",
        "# compress consecutive whitespaces\n",
        "regex_multiple_spaces = re.compile(r'\\s{2,}')\n",
        "# interrupted words usually have a \"- \" at the end before the new line, 'inter- rupted' -> 'interrupted'\n",
        "# NOTE: must be after whitespace compression\n",
        "regex_interrupted_word = re.compile(r'([a-zA-Z])- ')\n",
        "# remove page numbers, that are usually enclosed in characters like = or -, for example \"-12-\"\n",
        "regex_consecutive_meaningless_chars = re.compile(r'[^\\.a-zA-Z0-9\\s()]{2,} *(?:\\d+)?|(?P<prepage>[^a-zA-Z\\s\\d\\.])\\d+(?P=prepage)')\n",
        "# remove paragraphs id, '1.2.3' -> ''\n",
        "regex_dot_separated_digits = re.compile(r'(?:\\d+\\.)+\\d+')\n",
        "# remove meaningless chars after sentence start, '. (- start' -> '. start'\n",
        "regex_clean_start = re.compile(r'\\.(\\s?)[^a-zA-Z\\s]+')\n",
        "# recover missing apices\n",
        "regex_missing_apices = re.compile(r\"\\b([a-zA-Z]+) (t|s)\\b\")\n",
        "\n",
        "def clean_from_short_sentences(text: str) -> str:\n",
        "    return '.'.join(sentence for sentence in text.split('.') if regex_at_least_4_words_in_sentence.match(sentence) is not None)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    for clean_function in [lambda x: regex_mail.sub('', x),\n",
        "                           lambda x: regex_link.sub('', x),\n",
        "                           lambda x: regex_dot_separated_digits.sub('', x),\n",
        "                           lambda x: regex_consecutive_meaningless_chars.sub('', x),\n",
        "                           lambda x: regex_clean_start.sub(r'.\\1', x),\n",
        "                           # everything that is remove should be placed before this line so that \n",
        "                           # eventual spaces are compressed with regex_multiple_space\n",
        "                           lambda x: regex_multiple_spaces.sub(' ', x),\n",
        "                           lambda x: regex_interrupted_word.sub(r'\\1', x),\n",
        "                           lambda x: regex_missing_apices.sub(r\"\\1'\\2\", x),\n",
        "                           lambda x: clean_from_short_sentences(x),\n",
        "                           lambda x: regex_distance_between_period_and_following_word.sub('. ', x)]:\n",
        "        text = clean_function(text)\n",
        "    return text\n",
        "\n",
        "test_text = 'this is a test (me@gmail.it) -12- that wi-  ll be   cleaned. with 2 5 6 not valid. two sentences can t be good http://or.not.'\n",
        "cleaned_text = clean_text(test_text)\n",
        "print(cleaned_text)\n",
        "assert cleaned_text == 'this is a test () that will be cleaned. two sentences can\\'t be good '"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBHkUGAsukWJ",
        "outputId": "8a30f746-a6c1-4c50-9f24-001debd626da",
        "collapsed": true
      },
      "id": "bBHkUGAsukWJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a test () that will be cleaned. two sentences can't be good \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import pandas as pd\n",
        "import coreferee\n",
        "\n",
        "def remove_columns_prefix(df: pd.DataFrame) -> None:\n",
        "    '''remove prefix 'info.' from the columns of df'''\n",
        "    df.rename(columns=lambda c: c.rsplit('.', 1)[-1], inplace=True)\n",
        "\n",
        "def _get_new_token_from_resolve(token: spacy.tokens.Token, \n",
        "                                chains: coreferee.data_model.ChainHolder) -> spacy.tokens.Token:\n",
        "    resolved_token = chains.resolve(token)\n",
        "    return token.text_with_ws if resolved_token is None \\\n",
        "                              else 'and '.join([res_token.text_with_ws + ' ' for res_token in resolved_token])   \n",
        "\n",
        "def _process_doc_for_coref(doc: spacy.tokens.Doc) -> str:\n",
        "    replacement_tokens = []\n",
        "    chains = doc._.coref_chains\n",
        "    new_doc_tokens_text = [_get_new_token_from_resolve(token, chains) for token in doc]\n",
        "\n",
        "    return ''.join(new_doc_tokens_text)\n",
        "\n",
        "def preprocess_texts(texts: List[str]) -> List[str]:\n",
        "    nlp = spacy.load('en_core_web_trf')\n",
        "    nlp.add_pipe(\"coreferee\")\n",
        "\n",
        "    texts = [clean_text(text) for text in texts]\n",
        "    docs = nlp.pipe(texts)\n",
        "\n",
        "    return [regex_multiple_spaces.sub(' ', _process_doc_for_coref(doc)) for doc in docs]\n",
        "        \n",
        "text = get_document_by_line(DATASET_FILE_PATH, 103)\n",
        "# text = '''Although he was very busy with his work, the magical Peter had had enough of it. \n",
        "#     He and his wife decided they needed a holiday. \n",
        "#     this couple travelled to Spain because it loves the country very much.'''\n",
        "preprocess_texts([text])"
      ],
      "metadata": {
        "id": "jpT4BvSfFWTd"
      },
      "id": "jpT4BvSfFWTd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import Generator, Tuple\n",
        "import ast\n",
        "\n",
        "START_CLEANING = False\n",
        "CHUNK_SIZE = 20\n",
        "DATASET_ROWS = 381\n",
        "\n",
        "assert START_CLEANING == True # make sure you do not start preprocessing again\n",
        "\n",
        "def clean_data_row(row, docs_dict: Generator[Tuple[int, str], None, None]):\n",
        "    id_rulebook = next(docs_dict)\n",
        "    assert id_rulebook[0] == row['id']\n",
        "    row['rulebook'] = id_rulebook[1]\n",
        "    return row\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "for skip_rows in range(1, DATASET_ROWS, CHUNK_SIZE):\n",
        "    column_names = ['rulebook', 'info.id', 'info.name', 'info.averageweight', 'info.playingtime', 'info.family']\n",
        "    # ast.literal_eval converts the family column string into a python array\n",
        "    df_dataset = pd.read_csv(DATASET_FILE_PATH, converters={ 'info.family': ast.literal_eval },\n",
        "                            names=column_names, header=None,\n",
        "                            nrows=CHUNK_SIZE, skiprows=skip_rows)\n",
        "    remove_columns_prefix(df_dataset)\n",
        "    logger.info(f\"processing boardgames from {df_dataset.loc[0, 'id']} to {df_dataset.loc[df_dataset.index[-1], 'id']}\")\n",
        "    docs_dict = zip(df_dataset['id'].values, preprocess_texts(df_dataset['rulebook'].values))\n",
        "\n",
        "    df_cleaned_dataset = df_dataset.apply(lambda x: clean_data_row(x, docs_dict),\n",
        "                                        axis='columns')\n",
        "\n",
        "    df_cleaned_dataset.to_csv(CLEANED_DATASET_FILE_PATH, \n",
        "                            header=True if skip_rows == 1 else False, index=False, \n",
        "                            mode='w' if skip_rows == 1 else 'a')\n",
        "\n",
        "if not WORKING_LOCALLY:\n",
        "    drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "df_cleaned_dataset.head()"
      ],
      "metadata": {
        "id": "OfVY9Q2-TU02"
      },
      "id": "OfVY9Q2-TU02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Luck metrics\n",
        "these metrics are retrieved using rule-based matching and dependency matching. Luck is one of the criteria that determine the bg weight. In this case, the sources of luck considered are:\n",
        "\n",
        "- Dice rolling\n",
        "- Drawing\n",
        "- Shuffling\n",
        "- Words like *random* or *randomly*"
      ],
      "metadata": {
        "id": "KXWqWXyp3PVL"
      },
      "id": "KXWqWXyp3PVL"
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "from collections import namedtuple\n",
        "from spacy.matcher import Matcher, DependencyMatcher\n",
        "\n",
        "# TODO: flip a coin, flip sth like a coin\n",
        "LuckMetrics = namedtuple('LuckMetrics', ['dice_based', 'drawing_based', 'shuffling_based', 'random_based'])\n",
        "\n",
        "def get_luck_metrics(doc: spacy.tokens.Doc) -> LuckMetrics:\n",
        "    # ---------- random ----------\n",
        "    random_matcher = Matcher(doc.vocab)\n",
        "    random_patterns_match = [\n",
        "        [{\"LEMMA\": { \"IN\": [\"random\", \"randomly\"]}}]\n",
        "    ]\n",
        "    random_matcher.add(\"random\", random_patterns_match)\n",
        "\n",
        "    # ---------- shuffle ----------\n",
        "    shuffle_matcher = Matcher(doc.vocab)\n",
        "    shuffle_patterns_match = [\n",
        "        [{\"LEMMA\": \"shuffle\"}]\n",
        "    ]\n",
        "    shuffle_matcher.add(\"shuffle\", shuffle_patterns_match)\n",
        "\n",
        "    # ---------- card drawing ----------\n",
        "    drawing_matcher = DependencyMatcher(doc.vocab)    \n",
        "    drawing_patterns = [\n",
        "        [\n",
        "            {\n",
        "                \"RIGHT_ID\": \"drawing\",\n",
        "                \"RIGHT_ATTRS\": {\"LEMMA\": \"draw\", \"POS\": \"VERB\"}\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"drawing\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"card\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": \"card\",\n",
        "                    \"POS\": \"NOUN\", \n",
        "                    \"DEP\": { \"IN\": ['dobj', 'nsubjpass', 'compound'] }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "    drawing_matcher.add(\"drawing\", drawing_patterns)\n",
        "    # ---------- dice rolling ----------\n",
        "    dice_matcher = DependencyMatcher(doc.vocab)    \n",
        "    dice_patterns = [\n",
        "        [\n",
        "            {\n",
        "                \"RIGHT_ID\": \"rolling\",\n",
        "                \"RIGHT_ATTRS\": {\"LEMMA\": { \"IN\": [\"use\", \"throw\", \"roll\"]}, \"POS\": \"VERB\"}\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"rolling\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"dice_or_die\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"die\", \"dice\"]},\n",
        "                    \"POS\": \"NOUN\", \n",
        "                    \"DEP\": { \"IN\": ['nsubj', 'dobj', 'nsubjpass', 'compound'] }\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"RIGHT_ID\": \"rolling\",\n",
        "                \"RIGHT_ATTRS\": {\"LEMMA\": { \"IN\": [\"use\", \"throw\", \"roll\"]}, \"POS\": \"VERB\"}\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"rolling\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"number\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"IS_DIGIT\": True, \n",
        "                    \"DEP\": { \"IN\": ['dobj'] }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "    dice_matcher.add(\"diceroll\", dice_patterns)\n",
        "\n",
        "    dice_matches = dice_matcher(doc) \n",
        "    draw_matches = drawing_matcher(doc)\n",
        "    shuffle_matches = shuffle_matcher(doc)\n",
        "    random_matches = random_matcher(doc)\n",
        "\n",
        "    # TODO: needs normalization? (divide by rulebook length or tokens)\n",
        "\n",
        "    return LuckMetrics(len(dice_matches), len(draw_matches), len(shuffle_matches), len(random_matches))\n",
        "\n",
        "text = get_document_by_line(CLEANED_DATASET_FILE_PATH, 130)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(clean_text(text))\n",
        "print(len(doc), len(doc.text))\n",
        "print(get_luck_metrics(doc))\n",
        "\n",
        "# displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "id": "Ayy4X1vaYYCj",
        "outputId": "03ac40c0-9067-4e86-ed35-133dd3a053b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ayy4X1vaYYCj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2607 12624\n",
            "LuckMetrics(dice_based=12, drawing_based=4, shuffling_based=3, random_based=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = '''you can only take this because it can be outrageous. \n",
        "#     you can't take it. you could not also choose. you may never be sure of the result. \n",
        "#     you can decide the next thing. he has no other choice but to stop, another option is winning.'''\n",
        "\n",
        "text = '''In some cases Good and Goods could have different lemma'''\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "print([token.pos for token in doc])\n",
        "print(len(doc), len(doc.text))\n",
        "\n",
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "I3JtzI3p9lvj",
        "outputId": "34dc9ec1-bfda-4d9f-a7c5-ea07c022b8b0"
      },
      "id": "I3JtzI3p9lvj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[85, 90, 92, 84, 89, 96, 87, 100, 84, 92]\n",
            "10 55\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"da54ea8bd2394e2e914e0b03a32ce703-0\" class=\"displacy\" width=\"1800\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">In</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">some</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">cases</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Good</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Goods</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">could</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">have</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">different</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">lemma</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-da54ea8bd2394e2e914e0b03a32ce703-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-da54ea8bd2394e2e914e0b03a32ce703-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-da54ea8bd2394e2e914e0b03a32ce703-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-da54ea8bd2394e2e914e0b03a32ce703-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-da54ea8bd2394e2e914e0b03a32ce703-0-2\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-da54ea8bd2394e2e914e0b03a32ce703-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M395.0,266.5 L403.0,254.5 387.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-da54ea8bd2394e2e914e0b03a32ce703-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-da54ea8bd2394e2e914e0b03a32ce703-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-da54ea8bd2394e2e914e0b03a32ce703-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-da54ea8bd2394e2e914e0b03a32ce703-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-da54ea8bd2394e2e914e0b03a32ce703-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-da54ea8bd2394e2e914e0b03a32ce703-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-da54ea8bd2394e2e914e0b03a32ce703-0-6\" stroke-width=\"2px\" d=\"M595,264.5 C595,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-da54ea8bd2394e2e914e0b03a32ce703-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-da54ea8bd2394e2e914e0b03a32ce703-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-da54ea8bd2394e2e914e0b03a32ce703-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-da54ea8bd2394e2e914e0b03a32ce703-0-8\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,89.5 1620.0,89.5 1620.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-da54ea8bd2394e2e914e0b03a32ce703-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1620.0,266.5 L1628.0,254.5 1612.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Amount of choices metrics\n",
        "these metrics are retrieved using rule-based matching and dependency matching. The amount of choices every player has is one of the criteria that determine the bg weight. In this case, I am considering:\n",
        "\n",
        "- *can/could/may/decide/...*, with some exceptions:\n",
        "    - negatives are not considered choices, like *cannot draw* or *don't choose*\n",
        "    - *can* + *choose* and similar ones increase the *amount of choices* metrics by 1\n",
        "- *choice/option*, except when there is a leading *no*."
      ],
      "metadata": {
        "id": "vxB-jhmuFBrV"
      },
      "id": "vxB-jhmuFBrV"
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "from collections import namedtuple\n",
        "from spacy.matcher import Matcher, DependencyMatcher\n",
        "\n",
        "def get_choices_amount_metric(doc: spacy.tokens.Doc) -> int:\n",
        "    # --------------  can/could/may/choose/select/... -------------- \n",
        "    # all can/could/may\n",
        "    can_could_may_matcher = Matcher(doc.vocab)\n",
        "    can_could_may_patterns = [\n",
        "        [{\n",
        "            \"LEMMA\": { \"IN\": [\"can\", \"could\", \"may\", \"decide\", \"select\", \"choose\", \"opt\"]}, \n",
        "            \"POS\": { \"IN\": [\"AUX\", \"VERB\"]}\n",
        "        }]\n",
        "    ]\n",
        "    can_could_may_matcher.add('can_could_may', can_could_may_patterns)\n",
        "    can_could_may_matches = { match[1] for match in can_could_may_matcher(doc) }\n",
        "\n",
        "    # can/could/may with only or neg\n",
        "    can_could_may_exceptions_matcher = DependencyMatcher(doc.vocab)\n",
        "    can_could_may_exceptions_patterns = [\n",
        "        [\n",
        "            # ❌ can not/only/never verb \n",
        "            {\n",
        "                \"RIGHT_ID\": \"can_could_may\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"can\", \"could\", \"may\"]}, \n",
        "                    \"POS\": \"AUX\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"can_could_may\",\n",
        "                \"REL_OP\": \"<\",\n",
        "                \"RIGHT_ID\": \"generic_verb\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"POS\": { \"IN\": [\"AUX\", \"VERB\"] }\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"generic_verb\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"neg_or_only\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"not\", \"only\", \"never\"]}, \n",
        "                    \"DEP\": { \"IN\": [\"advmod\", \"neg\"] }\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            # ❌ not/only/never choose\n",
        "            {\n",
        "                \"RIGHT_ID\": \"decision_verb\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"decide\", \"select\", \"choose\", \"opt\"]}, \n",
        "                    \"POS\": \"VERB\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"decision_verb\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"negation\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"not\", \"only\", \"never\"]}, \n",
        "                    \"DEP\": { \"IN\": [\"advmod\", \"neg\"] }\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            # ❌ can + choose are counted as 1. can token is left out\n",
        "            {\n",
        "                \"RIGHT_ID\": \"can_could_may\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"can\", \"could\", \"may\"]}, \n",
        "                    \"POS\": \"AUX\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"can_could_may\",\n",
        "                \"REL_OP\": \"<\",\n",
        "                \"RIGHT_ID\": \"decision_verb\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"decide\", \"select\", \"choose\", \"opt\"]},\n",
        "                    \"POS\": \"VERB\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "    can_could_may_exceptions_matcher.add('can_could_may_exceptions', can_could_may_exceptions_patterns)\n",
        "    can_could_may_exceptions_matches = { match[1][0] for match in can_could_may_exceptions_matcher(doc) }\n",
        "\n",
        "    # -------------- choice and option -------------- \n",
        "    choice_option_matcher = Matcher(doc.vocab)\n",
        "    choice_option_patterns = [\n",
        "        [{\n",
        "            \"LEMMA\": { \"IN\": [\"choice\", \"option\"]}, \n",
        "            \"POS\": \"NOUN\"\n",
        "        }]\n",
        "    ]\n",
        "    choice_option_matcher.add('choice_option', choice_option_patterns)\n",
        "    choice_option_matches = { match[1] for match in choice_option_matcher(doc) }\n",
        "\n",
        "    choice_option_exceptions_matcher = DependencyMatcher(doc.vocab)\n",
        "    choice_option_exceptions_patterns = [\n",
        "        [\n",
        "            {\n",
        "                \"RIGHT_ID\": \"choice\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"choice\", \"option\"]}, \n",
        "                    \"POS\": \"NOUN\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"choice\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"prefix_no\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": \"no\",\n",
        "                    \"POS\": \"DET\",\n",
        "                    \"DEP\": \"det\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "    choice_option_exceptions_matcher.add('choice_option_exceptions', choice_option_exceptions_patterns)\n",
        "    choice_option_exceptions_matches = { match[1][0] for match in choice_option_exceptions_matcher(doc) }\n",
        "\n",
        "    return len(can_could_may_matches.difference(can_could_may_exceptions_matches)) + \\\n",
        "           len(choice_option_matches.difference(choice_option_exceptions_matches))\n",
        "\n",
        "text = get_document_by_line(CLEANED_DATASET_FILE_PATH, 130)\n",
        "# text = '''you can only take this because it can be outrageous. \n",
        "#     you can't take it. you can not also choose. you can never be sure of the result. \n",
        "#     you can decide the next thing, or you choose the target. another choice is to win. \n",
        "#     but there is no right option.'''\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(clean_text(text))\n",
        "print([(i, token.lemma_) for i, token in enumerate(doc)])\n",
        "print(len(doc), len(doc.text))\n",
        "print(get_choices_amount_metric(doc))\n",
        "\n",
        "# displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXvPodfk0ryj",
        "outputId": "1541ee2c-37d6-4499-953f-2767ad3dab6e"
      },
      "id": "vXvPodfk0ryj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 'page'), (1, '1'), (2, 'of'), (3, '4'), (4, '\"'), (5, 'wizardology'), (6, '\"'), (7, 'GAME'), (8, 'play'), (9, 'objective'), (10, ':'), (11, 'to'), (12, 'become'), (13, 'a'), (14, 'Master'), (15, 'Wizard'), (16, 'by'), (17, 'collect'), (18, '4'), (19, 'talisman'), (20, '('), (21, 'a'), (22, 'familiar'), (23, ','), (24, 'a'), (25, 'wizard'), (26, \"'s\"), (27, 'hat'), (28, ','), (29, 'a'), (30, 'staff'), (31, ','), (32, 'and'), (33, 'an'), (34, 'amulet'), (35, ')'), (36, 'and'), (37, 'free'), (38, 'Merlin'), (39, \"'s\"), (40, 'spirit'), (41, 'from'), (42, 'the'), (43, 'old'), (44, 'oak'), (45, 'tree'), (46, '.'), (47, 'place'), (48, 'the'), (49, 'maze'), (50, 'board'), (51, 'in'), (52, 'the'), (53, 'center'), (54, 'of'), (55, 'the'), (56, 'table'), (57, '.'), (58, 'place'), (59, 'a'), (60, 'Spirit'), (61, 'Chamber'), (62, 'at'), (63, 'each'), (64, 'of'), (65, 'the'), (66, '4'), (67, 'open'), (68, 'outer'), (69, 'doorway'), (70, 'of'), (71, 'the'), (72, 'game'), (73, 'board'), (74, '.'), (75, 'place'), (76, 'the'), (77, 'familiar'), (78, 'die'), (79, 'and'), (80, 'the'), (81, 'familiar'), (82, 'in'), (83, 'the'), (84, 'Water'), (85, 'Spirit'), (86, 'Chamber'), (87, '.'), (88, 'place'), (89, 'the'), (90, 'Dragon'), (91, 'Medallion'), (92, 'and'), (93, 'the'), (94, 'wizard'), (95, 'hat'), (96, 'in'), (97, 'the'), (98, 'Air'), (99, 'Spirit'), (100, 'Chamber'), (101, '.'), (102, 'place'), (103, 'the'), (104, 'cup'), (105, 'with'), (106, 'the'), (107, 'wizard'), (108, 'staff'), (109, 'inside'), (110, 'cup'), (111, 'in'), (112, 'the'), (113, 'Fire'), (114, 'Spirit'), (115, 'Chamber'), (116, '.'), (117, 'place'), (118, 'the'), (119, 'magic'), (120, 'wand'), (121, ','), (122, 'levitation'), (123, 'magnet'), (124, 'and'), (125, 'amulet'), (126, 'in'), (127, 'the'), (128, 'Earth'), (129, 'Spirit'), (130, 'Chamber'), (131, '.'), (132, 'each'), (133, 'player'), (134, 'select'), (135, 'a'), (136, 'wizard'), (137, 'whose'), (138, 'path'), (139, 'wizard'), (140, 'or'), (141, 'player'), (142, 'wish'), (143, 'to'), (144, 'follow'), (145, 'and'), (146, 'place'), (147, 'the'), (148, 'wizard'), (149, 'figure'), (150, 'in'), (151, 'the'), (152, 'center'), (153, 'of'), (154, 'the'), (155, 'maze'), (156, '.'), (157, 'shuffle'), (158, 'the'), (159, 'Magical'), (160, 'Item'), (161, 'Cards'), (162, ','), (163, 'Crystal'), (164, 'Ball'), (165, 'Cards'), (166, ','), (167, 'and'), (168, 'Phoenix'), (169, 'Feather'), (170, 'Cards'), (171, '.'), (172, 'keep'), (173, 'each'), (174, 'deck'), (175, 'separate'), (176, 'and'), (177, 'put'), (178, 'Cards'), (179, 'and'), (180, 'Cards'), (181, 'and'), (182, 'Cards'), (183, 'to'), (184, 'the'), (185, 'side'), (186, '.'), (187, 'place'), (188, 'the'), (189, 'Spells'), (190, 'and'), (191, 'Potions'), (192, 'book'), (193, 'and'), (194, 'dice'), (195, 'off'), (196, 'to'), (197, 'the'), (198, 'side'), (199, '.'), (200, 'play'), (201, 'the'), (202, 'GAME'), (203, ':'), (204, 'your'), (205, 'goal'), (206, 'be'), (207, 'to'), (208, 'collect'), (209, 'all'), (210, '4'), (211, 'talisman'), (212, '.'), (213, 'you'), (214, 'do'), (215, 'this'), (216, 'by'), (217, 'successfully'), (218, 'move'), (219, 'around'), (220, 'the'), (221, 'maze'), (222, 'and'), (223, 'collect'), (224, 'card'), (225, 'that'), (226, 'will'), (227, 'help'), (228, 'you'), (229, 'collect'), (230, 'the'), (231, 'talisman'), (232, '.'), (233, 'you'), (234, 'may'), (235, 'also'), (236, 'duel'), (237, 'other'), (238, 'apprentice'), (239, 'for'), (240, 'apprentice'), (241, 'card'), (242, '.'), (243, 'each'), (244, 'apprentice'), (245, 'roll'), (246, 'the'), (247, 'double'), (248, 'die'), (249, '.'), (250, 'the'), (251, 'high'), (252, 'roller'), (253, 'go'), (254, 'first'), (255, 'and'), (256, 'play'), (257, 'continue'), (258, 'clockwise'), (259, '.'), (260, 'the'), (261, 'board'), (262, 'be'), (263, 'a'), (264, 'maze'), (265, ','), (266, 'so'), (267, 'the'), (268, 'first'), (269, 'challenge'), (270, 'lie'), (271, 'in'), (272, 'choose'), (273, 'which'), (274, 'direction'), (275, 'to'), (276, 'take'), (277, '.'), (278, 'the'), (279, 'center'), (280, ','), (281, 'as'), (282, 'well'), (283, 'as'), (284, 'the'), (285, 'different'), (286, 'ring'), (287, 'of'), (288, 'the'), (289, 'maze'), (290, 'be'), (291, 'mark'), (292, 'with'), (293, 'open'), (294, 'doorway'), (295, 'to'), (296, 'enter'), (297, 'or'), (298, 'leave'), (299, 'by'), (300, '.'), (301, 'each'), (302, 'apprentice'), (303, 'must'), (304, 'find'), (305, 'apprentice'), (306, 'or'), (307, 'roller'), (308, 'way'), (309, 'through'), (310, 'the'), (311, 'maze'), (312, 'to'), (313, 'visit'), (314, 'each'), (315, 'of'), (316, 'the'), (317, '4'), (318, 'outer'), (319, 'Spirit'), (320, 'Chambers'), (321, 'to'), (322, 'collect'), (323, 'the'), (324, 'talisman'), (325, '.'), (326, 'there'), (327, 'be'), (328, 'no'), (329, 'order'), (330, 'in'), (331, 'which'), (332, 'the'), (333, 'Spirit'), (334, 'Chambers'), (335, 'must'), (336, 'be'), (337, 'visit'), (338, '.'), (339, 'when'), (340, 'navigate'), (341, 'the'), (342, 'maze'), (343, ','), (344, 'if'), (345, 'an'), (346, 'apprentice'), (347, 'reach'), (348, 'a'), (349, 'dead'), (350, '-'), (351, 'end'), (352, ','), (353, 'apprentice'), (354, 'or'), (355, 'roller'), (356, 'may'), (357, 'bounce'), (358, 'off'), (359, 'the'), (360, 'wall'), (361, 'to'), (362, 'continue'), (363, 'move'), (364, '.'), (365, 'for'), (366, 'example'), (367, ','), (368, 'if'), (369, 'you'), (370, 'roll'), (371, 'a'), (372, '5'), (373, 'and'), (374, 'be'), (375, 'only'), (376, '2'), (377, 'space'), (378, 'from'), (379, 'the'), (380, 'dead'), (381, '-'), (382, 'end'), (383, ','), (384, 'you'), (385, 'may'), (386, 'move'), (387, '2'), (388, 'space'), (389, 'then'), (390, 'bounce'), (391, 'back'), (392, 'for'), (393, 'move'), (394, '3'), (395, ','), (396, '4'), (397, ','), (398, 'and'), (399, '5'), (400, '.'), (401, 'dead'), (402, '-'), (403, 'end'), (404, 'be'), (405, 'normally'), (406, 'lock'), (407, ','), (408, 'but'), (409, 'can'), (410, 'be'), (411, 'open'), (412, 'if'), (413, 'you'), (414, 'have'), (415, 'the'), (416, '\"'), (417, 'Open'), (418, 'Sesame'), (419, '\"'), (420, 'Card'), (421, '.'), (422, 'the'), (423, 'spaces'), (424, 'on'), (425, 'the'), (426, 'BOARD'), (427, ':'), (428, '1'), (429, '.'), (430, 'Magical'), (431, 'Items'), (432, ':'), (433, 'draw'), (434, 'one'), (435, 'card'), (436, 'from'), (437, 'the'), (438, 'Magic'), (439, 'Item'), (440, 'deck'), (441, '.'), (442, 'Magical'), (443, 'Item'), (444, 'Cards'), (445, 'be'), (446, 'key'), (447, 'to'), (448, 'enter'), (449, 'the'), (450, 'Spirit'), (451, 'Chambers'), (452, '-'), (453, 'you'), (454, 'can'), (455, 'not'), (456, 'gain'), (457, 'access'), (458, 'to'), (459, 'a'), (460, 'particular'), (461, 'chamber'), (462, 'without'), (463, 'the'), (464, 'card'), (465, 'that'), (466, 'correspond'), (467, 'to'), (468, 'chamber'), (469, '.'), (470, 'the'), (471, 'more'), (472, 'Magical'), (473, 'Item'), (474, 'Cards'), (475, 'an'), (476, 'apprentice'), (477, 'have'), (478, ','), (479, 'the'), (480, 'more'), (481, 'chance'), (482, 'there'), (483, 'will'), (484, 'be'), (485, 'to'), (486, 'successfully'), (487, 'attempt'), (488, 'each'), (489, 'task'), (490, '.'), (491, 'Animal'), (492, 'Magic'), (493, 'Guide'), (494, 'Book'), (495, ':'), (496, 'the'), (497, 'key'), (498, 'to'), (499, 'enter'), (500, 'the'), (501, 'Water'), (502, 'Spirit'), (503, 'Chamber'), (504, 'Dragon'), (505, 'Medallion'), (506, ':'), (507, 'the'), (508, 'key'), (509, 'to'), (510, 'enter'), (511, 'the'), (512, 'Air'), (513, 'Spirit'), (514, 'Chamber'), (515, '.'), (516, 'athame'), (517, ':'), (518, 'key'), (519, 'to'), (520, 'enter'), (521, 'the'), (522, 'Fire'), (523, 'Spirit'), (524, 'Chamber'), (525, 'Ring'), (526, 'of'), (527, 'Power'), (528, ':'), (529, 'key'), (530, 'to'), (531, 'enter'), (532, 'the'), (533, 'Earth'), (534, 'Spirit'), (535, 'Chamber'), (536, '.'), (537, 'Wild'), (538, 'Cards'), (539, ':'), (540, 'these'), (541, 'card'), (542, 'can'), (543, 'be'), (544, 'use'), (545, 'to'), (546, 'represent'), (547, 'any'), (548, 'of'), (549, 'the'), (550, '4'), (551, 'Magical'), (552, 'Items'), (553, 'you'), (554, 'wish'), (555, '.'), (556, 'release'), (557, 'a'), (558, 'Genie'), (559, ':'), (560, 'roll'), (561, 'the'), (562, 'double'), (563, 'die'), (564, '.'), (565, 'if'), (566, 'the'), (567, 'outer'), (568, 'die'), (569, 'be'), (570, 'high'), (571, 'than'), (572, 'the'), (573, 'inner'), (574, 'die'), (575, ','), (576, 'the'), (577, 'apprentice'), (578, 'have'), (579, 'successfully'), (580, 'release'), (581, 'a'), (582, 'genie'), (583, '.'), (584, 'apprentice'), (585, 'or'), (586, 'Chamber'), (587, 'then'), (588, 'move'), (589, 'to'), (590, 'the'), (591, 'Spirit'), (592, 'Chamber'), (593, 'name'), (594, 'on'), (595, 'that'), (596, 'card'), (597, '.'), (598, 'if'), (599, 'the'), (600, 'apprentice'), (601, 'have'), (602, 'the'), (603, 'Magical'), (604, 'Item'), (605, 'Card'), (606, 'for'), (607, 'that'), (608, 'particular'), (609, 'Spirit'), (610, 'Chamber'), (611, ','), (612, 'apprentice'), (613, 'can'), (614, 'perform'), (615, 'the'), (616, 'task'), (617, 'require'), (618, 'to'), (619, 'earn'), (620, 'a'), (621, 'talisman'), (622, '.'), (623, 'if'), (624, 'the'), (625, 'apprentice'), (626, 'do'), (627, 'not'), (628, 'have'), (629, 'the'), (630, 'Magical'), (631, 'Item'), (632, 'Card'), (633, ','), (634, 'the'), (635, 'turn'), (636, 'be'), (637, 'over'), (638, '.'), (639, 'apprentice'), (640, 'leave'), (641, 'the'), (642, 'Spirit'), (643, 'Chamber'), (644, 'on'), (645, 'apprentice'), (646, 'next'), (647, 'turn'), (648, '.'), (649, 'if'), (650, 'the'), (651, 'inner'), (652, 'die'), (653, 'be'), (654, 'high'), (655, 'than'), (656, 'the'), (657, 'outer'), (658, ','), (659, 'or'), (660, 'a'), (661, 'tie'), (662, 'occur'), (663, ','), (664, 'the'), (665, 'apprentice'), (666, 'have'), (667, 'fail'), (668, 'to'), (669, 'release'), (670, 'the'), (671, 'genie'), (672, 'and'), (673, 'the'), (674, 'turn'), (675, 'be'), (676, 'over'), (677, '.'), (678, 'Fairy'), (679, 'Flag'), (680, ':'), (681, 'if'), (682, 'you'), (683, 'draw'), (684, 'this'), (685, 'card'), (686, ','), (687, 'you'), (688, 'instantly'), (689, 'earn'), (690, 'a'), (691, 'talisman'), (692, 'of'), (693, 'choice'), (694, '.'), (695, 'there'), (696, 'be'), (697, 'no'), (698, 'need'), (699, 'to'), (700, 'go'), (701, 'to'), (702, 'the'), (703, 'Spirit'), (704, 'Chamber'), (705, 'or'), (706, 'perform'), (707, 'the'), (708, 'task'), (709, '!'), (710, '5'), (711, '.'), (712, 'hand'), (713, 'of'), (714, 'glory'), (715, ':'), (716, 'this'), (717, 'allow'), (718, 'an'), (719, 'apprentice'), (720, 'to'), (721, 'steal'), (722, 'a'), (723, 'talisman'), (724, 'from'), (725, 'any'), (726, 'other'), (727, 'player'), (728, '.'), (729, 'take'), (730, 'the'), (731, 'talisman'), (732, 'of'), (733, 'choice'), (734, 'from'), (735, 'another'), (736, 'player'), (737, ','), (738, 'return'), (739, 'talisman'), (740, 'to'), (741, 'the'), (742, 'Spirit'), (743, 'Chamber'), (744, 'talisman'), (745, 'come'), (746, 'from'), (747, ','), (748, 'then'), (749, 'take'), (750, 'your'), (751, 'corresponding'), (752, 'talisman'), (753, '.'), (754, 'Kee'), (755, \"'s\"), (756, 'Locator'), (757, ':'), (758, 'if'), (759, 'a'), (760, 'Hand'), (761, 'of'), (762, 'Glory'), (763, 'card'), (764, 'be'), (765, 'use'), (766, 'on'), (767, 'you'), (768, 'to'), (769, 'steal'), (770, 'one'), (771, 'of'), (772, 'your'), (773, 'talisman'), (774, ','), (775, 'this'), (776, 'card'), (777, 'will'), (778, 'help'), (779, 'get'), (780, 'one'), (781, 'back'), (782, '.'), (783, 'you'), (784, 'must'), (785, 'return'), (786, 'to'), (787, 'the'), (788, 'Spirit'), (789, 'Chamber'), (790, 'of'), (791, 'the'), (792, 'talisman'), (793, 'that'), (794, 'be'), (795, 'steal'), (796, 'and'), (797, 'present'), (798, 'this'), (799, 'card'), (800, '.'), (801, 'you'), (802, 'will'), (803, 'not'), (804, 'need'), (805, 'to'), (806, 'perfrom'), (807, 'the'), (808, 'task'), (809, '-'), (810, 'the'), (811, 'steal'), (812, 'talisman'), (813, 'will'), (814, 'be'), (815, 'instantly'), (816, 'return'), (817, '.'), (818, 'happiness'), (819, 'Spell'), (820, ':'), (821, 'this'), (822, 'be'), (823, 'an'), (824, '\"'), (825, 'all'), (826, 'play'), (827, '\"'), (828, 'card'), (829, '.'), (830, 'each'), (831, 'apprentice'), (832, 'roll'), (833, 'the'), (834, 'double'), (835, 'die'), (836, '.'), (837, 'whoever'), (838, 'roll'), (839, 'the'), (840, 'high'), (841, 'combination'), (842, 'will'), (843, 'be'), (844, 'the'), (845, 'happy'), (846, 'and'), (847, 'be'), (848, 'allow'), (849, 'to'), (850, 'move'), (851, 'that'), (852, 'number'), (853, 'of'), (854, 'space'), (855, '.'), (856, 'play'), (857, 'continue'), (858, 'clockwise'), (859, 'from'), (860, 'the'), (861, 'winner'), (862, '.'), (863, 'dowsing'), (864, ':'), (865, 'if'), (866, 'you'), (867, 'pick'), (868, 'this'), (869, 'card'), (870, ','), (871, 'you'), (872, 'must'), (873, 'blind'), (874, 'draw'), (875, 'a'), (876, 'Magical'), (877, 'Item'), (878, 'Card'), (879, 'from'), (880, 'an'), (881, 'opponent'), (882, 'of'), (883, 'choice'), (884, '.'), (885, 'open'), (886, 'Sesame'), (887, ':'), (888, 'this'), (889, 'card'), (890, 'allow'), (891, 'an'), (892, 'apprentice'), (893, 'to'), (894, 'move'), (895, 'directly'), (896, 'through'), (897, 'a'), (898, 'wall'), (899, 'mark'), (900, 'with'), (901, 'a'), (902, 'lock'), (903, '.'), (904, 'once'), (905, 'the'), (906, 'player'), (907, 'have'), (908, 'unlock'), (909, 'a'), (910, 'wall'), (911, 'and'), (912, 'go'), (913, 'through'), (914, ','), (915, 'wall'), (916, 'becomes'), (917, 'lock'), (918, 'again'), (919, '.'), (920, 'Vivienne'), (921, \"'s\"), (922, 'Circle'), (923, ':'), (924, 'a'), (925, 'powerful'), (926, 'card'), (927, 'to'), (928, 'have'), (929, ','), (930, 'as'), (931, 'wall'), (932, 'protect'), (933, 'you'), (934, 'from'), (935, 'the'), (936, 'follow'), (937, 'magical'), (938, 'attack'), (939, ':'), (940, 'dowsing'), (941, ','), (942, 'Hand'), (943, 'of'), (944, 'Glory'), (945, ','), (946, 'Magical'), (947, 'Duels'), (948, 'and'), (949, 'Elf'), (950, 'Charms'), (951, '.'), (952, 'Wind'), (953, 'Knot'), (954, ':'), (955, 'this'), (956, 'card'), (957, 'can'), (958, 'really'), (959, 'give'), (960, 'you'), (961, 'the'), (962, 'advantage'), (963, 'against'), (964, 'your'), (965, 'opponent'), (966, '.'), (967, 'card'), (968, 'allow'), (969, 'an'), (970, 'apprentice'), (971, 'to'), (972, 'rotate'), (973, 'a'), (974, 'quarter'), (975, 'of'), (976, 'the'), (977, 'way'), (978, 'in'), (979, 'either'), (980, 'direction'), (981, '.'), (982, 'whether'), (983, 'you'), (984, 'rotate'), (985, 'to'), (986, 'the'), (987, 'right'), (988, 'or'), (989, 'to'), (990, 'the'), (991, 'left'), (992, ','), (993, 'you'), (994, 'must'), (995, 'always'), (996, 'make'), (997, 'sure'), (998, 'that'), (999, 'the'), (1000, 'doorway'), (1001, 'line'), (1002, 'up'), (1003, 'with'), (1004, 'a'), (1005, 'Spirit'), (1006, 'Chamber'), (1007, '.'), (1008, 'this'), (1009, 'allow'), (1010, 'player'), (1011, 'to'), (1012, 'move'), (1013, 'player'), (1014, 'close'), (1015, 'to'), (1016, 'a'), (1017, 'Spirit'), (1018, 'Chamber'), (1019, 'player'), (1020, 'may'), (1021, 'need'), (1022, 'to'), (1023, 'get'), (1024, 'to'), (1025, ','), (1026, 'or'), (1027, 'to'), (1028, 'move'), (1029, 'an'), (1030, 'opponent'), (1031, 'far'), (1032, 'away'), (1033, 'from'), (1034, 'one'), (1035, '.'), (1036, 'false'), (1037, 'Prophecy'), (1038, ':'), (1039, 'if'), (1040, 'you'), (1041, 'draw'), (1042, 'this'), (1043, 'card'), (1044, ','), (1045, 'you'), (1046, 'must'), (1047, 'start'), (1048, 'over'), (1049, 'again'), (1050, 'from'), (1051, 'the'), (1052, 'center'), (1053, 'of'), (1054, 'the'), (1055, 'maze'), (1056, '.'), (1057, 'you'), (1058, 'still'), (1059, 'keep'), (1060, 'everything'), (1061, 'you'), (1062, 'have'), (1063, 'collect'), (1064, 'so'), (1065, 'far'), (1066, '.'), (1067, 'Phoenix'), (1068, 'feather'), (1069, ':'), (1070, 'draw'), (1071, 'one'), (1072, 'card'), (1073, 'from'), (1074, 'the'), (1075, 'Phoenix'), (1076, 'Feather'), (1077, 'deck'), (1078, '.'), (1079, 'these'), (1080, 'card'), (1081, 'help'), (1082, 'transport'), (1083, 'apprentice'), (1084, 'around'), (1085, 'the'), (1086, 'maze'), (1087, 'more'), (1088, 'quickly'), (1089, '.'), (1090, 'Phoenix'), (1091, 'Feather'), (1092, 'Cards'), (1093, 'do'), (1094, 'not'), (1095, 'have'), (1096, 'to'), (1097, 'be'), (1098, 'use'), (1099, 'right'), (1100, 'away'), (1101, ','), (1102, 'but'), (1103, 'can'), (1104, 'be'), (1105, 'keep'), (1106, 'for'), (1107, 'later'), (1108, 'use'), (1109, '.'), (1110, 'there'), (1111, 'be'), (1112, '3'), (1113, 'different'), (1114, 'type'), (1115, 'of'), (1116, 'flight'), (1117, ':'), (1118, 'seven'), (1119, 'League'), (1120, 'boot'), (1121, ':'), (1122, 'this'), (1123, 'card'), (1124, 'allow'), (1125, 'an'), (1126, 'apprentice'), (1127, 'to'), (1128, 'travel'), (1129, 'to'), (1130, 'any'), (1131, 'space'), (1132, 'on'), (1133, 'the'), (1134, 'ring'), (1135, 'apprentice'), (1136, 'or'), (1137, 'card'), (1138, 'currently'), (1139, 'occupy'), (1140, '.'), (1141, 'the'), (1142, 'boot'), (1143, 'have'), (1144, 'limit'), (1145, ','), (1146, 'though'), (1147, '.'), (1148, 'if'), (1149, 'a'), (1150, 'wall'), (1151, 'be'), (1152, 'in'), (1153, 'your'), (1154, 'way'), (1155, ','), (1156, 'you'), (1157, 'can'), (1158, 'not'), (1159, 'go'), (1160, 'through'), (1161, 'wall'), (1162, ','), (1163, 'unless'), (1164, 'you'), (1165, 'possess'), (1166, 'an'), (1167, '\"'), (1168, 'Open'), (1169, 'Sesame'), (1170, '\"'), (1171, 'Card'), (1172, '.'), (1173, 'Broomstick'), (1174, ':'), (1175, 'before'), (1176, 'be'), (1177, 'able'), (1178, 'to'), (1179, 'use'), (1180, 'a'), (1181, 'Broomstick'), (1182, 'Card'), (1183, ','), (1184, 'the'), (1185, 'apprentice'), (1186, 'must'), (1187, 'roll'), (1188, 'the'), (1189, 'die'), (1190, '.'), (1191, 'if'), (1192, 'an'), (1193, 'even'), (1194, 'number'), (1195, 'be'), (1196, 'roll'), (1197, ','), (1198, 'the'), (1199, 'broomstick'), (1200, \"'s\"), (1201, 'magic'), (1202, 'be'), (1203, 'work'), (1204, 'in'), (1205, 'the'), (1206, 'apprentice'), (1207, \"'\"), (1208, 'favor'), (1209, 'and'), (1210, 'movement'), (1211, 'to'), (1212, 'anywhere'), (1213, 'in'), (1214, 'the'), (1215, 'maze'), (1216, ','), (1217, 'include'), (1218, 'the'), (1219, '4'), (1220, 'Spirit'), (1221, 'Chambers'), (1222, ','), (1223, 'be'), (1224, 'allow'), (1225, '.'), (1226, 'an'), (1227, 'odd'), (1228, 'number'), (1229, 'prove'), (1230, 'the'), (1231, 'broomstick'), (1232, 'useless'), (1233, '-'), (1234, 'the'), (1235, 'card'), (1236, 'should'), (1237, 'be'), (1238, 'discard'), (1239, 'and'), (1240, 'the'), (1241, 'apprentice'), (1242, 'turn'), (1243, 'be'), (1244, 'over'), (1245, '.'), (1246, 'Magic'), (1247, 'Carpet'), (1248, ':'), (1249, 'the'), (1250, 'good'), (1251, 'mean'), (1252, 'of'), (1253, 'flight'), (1254, '!'), (1255, 'an'), (1256, 'apprentice'), (1257, 'can'), (1258, 'fly'), (1259, 'anywhere'), (1260, 'on'), (1261, 'the'), (1262, 'maze'), (1263, ','), (1264, 'include'), (1265, 'the'), (1266, 'Spirit'), (1267, 'Chambers'), (1268, '.'), (1269, 'if'), (1270, 'an'), (1271, 'apprentice'), (1272, 'be'), (1273, 'wise'), (1274, 'apprentice'), (1275, 'or'), (1276, 'apprentice'), (1277, 'will'), (1278, 'hold'), (1279, 'onto'), (1280, 'at'), (1281, 'least'), (1282, 'one'), (1283, 'Magic'), (1284, 'Carpet'), (1285, 'Card'), (1286, 'during'), (1287, 'the'), (1288, 'course'), (1289, 'of'), (1290, 'the'), (1291, 'game'), (1292, '.'), (1293, 'this'), (1294, 'will'), (1295, 'prove'), (1296, 'to'), (1297, 'be'), (1298, 'quite'), (1299, 'useful'), (1300, 'when'), (1301, 'return'), (1302, 'to'), (1303, 'the'), (1304, 'enter'), (1305, 'for'), (1306, 'the'), (1307, 'final'), (1308, 'task'), (1309, 'of'), (1310, 'free'), (1311, 'Merlin'), (1312, '!'), (1313, 'page'), (1314, '3'), (1315, 'of'), (1316, '4'), (1317, '14'), (1318, '.'), (1319, 'roll'), (1320, 'double'), (1321, 'die'), (1322, ':'), (1323, 'roll'), (1324, 'the'), (1325, 'double'), (1326, 'die'), (1327, 'and'), (1328, 'move'), (1329, 'the'), (1330, 'combine'), (1331, 'amount'), (1332, 'of'), (1333, 'the'), (1334, 'inner'), (1335, 'and'), (1336, 'outer'), (1337, 'die'), (1338, '.'), (1339, 'lose'), (1340, 'a'), (1341, 'turn'), (1342, ':'), (1343, 'the'), (1344, 'apprentice'), (1345, 'lose'), (1346, 'apprentice'), (1347, 'turn'), (1348, '.'), (1349, 'Spells'), (1350, 'and'), (1351, 'Potions'), (1352, ':'), (1353, 'an'), (1354, 'apprentice'), (1355, 'must'), (1356, 'roll'), (1357, 'the'), (1358, 'Spells'), (1359, 'and'), (1360, 'Potions'), (1361, 'dice'), (1362, 'after'), (1363, 'land'), (1364, 'here'), (1365, '.'), (1366, 'one'), (1367, 'die'), (1368, 'be'), (1369, 'mark'), (1370, 'with'), (1371, 'a'), (1372, 'symbol'), (1373, ','), (1374, 'the'), (1375, 'other'), (1376, 'with'), (1377, 'a'), (1378, 'color'), (1379, '.'), (1380, 'the'), (1381, 'combination'), (1382, 'determine'), (1383, 'which'), (1384, 'spell'), (1385, 'or'), (1386, 'potion'), (1387, 'you'), (1388, 'have'), (1389, 'perform'), (1390, 'and'), (1391, 'whether'), (1392, 'or'), (1393, 'not'), (1394, 'you'), (1395, 'have'), (1396, 'perform'), (1397, 'spell'), (1398, 'successfully'), (1399, '.'), (1400, 'refer'), (1401, 'to'), (1402, 'the'), (1403, 'Spells'), (1404, 'and'), (1405, 'Potions'), (1406, 'book'), (1407, 'to'), (1408, 'look'), (1409, 'up'), (1410, 'your'), (1411, 'combination'), (1412, 'and'), (1413, 'find'), (1414, 'out'), (1415, 'the'), (1416, 'result'), (1417, 'of'), (1418, 'your'), (1419, 'roll'), (1420, '.'), (1421, 'the'), (1422, 'Alchemy'), (1423, 'lab'), (1424, ':'), (1425, 'roll'), (1426, 'certain'), (1427, 'combination'), (1428, 'on'), (1429, 'the'), (1430, 'Spells'), (1431, 'and'), (1432, 'Potions'), (1433, 'dice'), (1434, 'will'), (1435, 'land'), (1436, 'an'), (1437, 'apprentice'), (1438, 'in'), (1439, 'the'), (1440, 'Alchemy'), (1441, 'Lab'), (1442, '.'), (1443, 'on'), (1444, 'apprentice'), (1445, 'or'), (1446, 'lab'), (1447, 'next'), (1448, 'turn'), (1449, ','), (1450, 'the'), (1451, 'apprentice'), (1452, 'must'), (1453, 'attempt'), (1454, 'to'), (1455, 'escape'), (1456, 'by'), (1457, 'roll'), (1458, 'the'), (1459, 'same'), (1460, 'combination'), (1461, 'on'), (1462, 'the'), (1463, 'Spells'), (1464, 'and'), (1465, 'Potions'), (1466, 'dice'), (1467, 'that'), (1468, 'have'), (1469, 'originally'), (1470, 'send'), (1471, 'apprentice'), (1472, 'or'), (1473, 'lab'), (1474, 'here'), (1475, '.'), (1476, 'an'), (1477, 'apprentice'), (1478, 'have'), (1479, 'only'), (1480, '3'), (1481, 'chance'), (1482, 'per'), (1483, 'turn'), (1484, 'to'), (1485, 'get'), (1486, 'out'), (1487, '.'), (1488, 'apprentice'), (1489, 'or'), (1490, 'apprentice'), (1491, 'must'), (1492, 'stay'), (1493, 'there'), (1494, 'for'), (1495, 'as'), (1496, 'many'), (1497, 'turn'), (1498, 'as'), (1499, 'turn'), (1500, 'take'), (1501, 'to'), (1502, 'get'), (1503, 'out'), (1504, '.'), (1505, 'if'), (1506, 'a'), (1507, 'player'), (1508, 'land'), (1509, 'here'), (1510, 'during'), (1511, 'the'), (1512, 'normal'), (1513, 'course'), (1514, 'of'), (1515, 'play'), (1516, 'it'), (1517, 'should'), (1518, 'be'), (1519, 'treat'), (1520, 'as'), (1521, 'a'), (1522, 'blank'), (1523, 'space'), (1524, '.'), (1525, 'secret'), (1526, 'path'), (1527, ':'), (1528, 'if'), (1529, 'an'), (1530, 'apprentice'), (1531, 'land'), (1532, 'on'), (1533, 'a'), (1534, 'secret'), (1535, 'Path'), (1536, 'space'), (1537, ','), (1538, 'apprentice'), (1539, 'or'), (1540, 'player'), (1541, 'may'), (1542, 'move'), (1543, 'directly'), (1544, 'to'), (1545, 'any'), (1546, 'other'), (1547, 'secret'), (1548, 'Path'), (1549, 'space'), (1550, 'mark'), (1551, 'on'), (1552, 'the'), (1553, 'maze'), (1554, '.'), (1555, 'the'), (1556, 'apprentice'), (1557, 'will'), (1558, 'begin'), (1559, 'from'), (1560, 'this'), (1561, 'new'), (1562, 'space'), (1563, 'on'), (1564, 'apprentice'), (1565, 'or'), (1566, 'player'), (1567, 'next'), (1568, 'turn'), (1569, '.'), (1570, 'Crystal'), (1571, 'Ball'), (1572, ':'), (1573, 'an'), (1574, 'apprentice'), (1575, 'must'), (1576, 'draw'), (1577, 'from'), (1578, 'the'), (1579, 'Crystal'), (1580, 'Ball'), (1581, 'deck'), (1582, 'after'), (1583, 'land'), (1584, 'on'), (1585, 'this'), (1586, 'space'), (1587, '.'), (1588, 'it'), (1589, 'be'), (1590, 'not'), (1591, 'always'), (1592, 'wise'), (1593, 'to'), (1594, 'consult'), (1595, 'a'), (1596, 'Crystal'), (1597, 'Ball'), (1598, '.'), (1599, 'some'), (1600, 'thing'), (1601, 'you'), (1602, 'see'), (1603, 'in'), (1604, 'the'), (1605, 'ball'), (1606, 'may'), (1607, 'be'), (1608, 'good'), (1609, ','), (1610, 'some'), (1611, 'may'), (1612, 'be'), (1613, 'bad'), (1614, '.'), (1615, 'draw'), (1616, 'from'), (1617, 'this'), (1618, 'deck'), (1619, 'at'), (1620, 'your'), (1621, 'own'), (1622, 'risk'), (1623, '!'), (1624, '20'), (1625, '.'), (1626, 'Elf'), (1627, 'Charm'), (1628, ':'), (1629, 'if'), (1630, 'you'), (1631, 'pick'), (1632, 'this'), (1633, 'card'), (1634, ','), (1635, 'you'), (1636, 'must'), (1637, 'blind'), (1638, 'draw'), (1639, 'a'), (1640, 'Phoenix'), (1641, 'Feather'), (1642, 'Card'), (1643, 'from'), (1644, 'an'), (1645, 'apprentice'), (1646, 'of'), (1647, 'choice'), (1648, '.'), (1649, 'the'), (1650, 'TASKS'), (1651, ':'), (1652, 'when'), (1653, 'you'), (1654, 'reach'), (1655, 'a'), (1656, 'Spirit'), (1657, 'Chamber'), (1658, ','), (1659, 'you'), (1660, 'immediately'), (1661, 'perform'), (1662, 'the'), (1663, 'task'), (1664, 'require'), (1665, 'to'), (1666, 'earn'), (1667, 'that'), (1668, 'particular'), (1669, 'talisman'), (1670, '.'), (1671, 'familiar'), (1672, ':'), (1673, 'in'), (1674, 'the'), (1675, 'Water'), (1676, 'Spirit'), (1677, 'Chamber'), (1678, 'you'), (1679, 'call'), (1680, 'upon'), (1681, 'the'), (1682, 'playful'), (1683, 'spirit'), (1684, ','), (1685, 'Gladde'), (1686, ','), (1687, 'to'), (1688, 'help'), (1689, 'you'), (1690, 'summon'), (1691, 'your'), (1692, 'Familiar'), (1693, '.'), (1694, 'use'), (1695, 'the'), (1696, 'double'), (1697, 'die'), (1698, 'and'), (1699, 'the'), (1700, 'Familiar'), (1701, 'die'), (1702, 'for'), (1703, 'this'), (1704, 'task'), (1705, '.'), (1706, 'first'), (1707, 'roll'), (1708, 'the'), (1709, 'double'), (1710, 'die'), (1711, '.'), (1712, 'the'), (1713, 'outer'), (1714, 'die'), (1715, 'will'), (1716, 'determine'), (1717, 'the'), (1718, 'amount'), (1719, 'of'), (1720, 'chance'), (1721, 'you'), (1722, 'have'), (1723, 'to'), (1724, 'roll'), (1725, 'the'), (1726, 'Familiar'), (1727, 'die'), (1728, '.'), (1729, 'for'), (1730, 'example'), (1731, ','), (1732, 'if'), (1733, 'you'), (1734, 'be'), (1735, 'the'), (1736, 'Lapp'), (1737, 'Shaman'), (1738, 'and'), (1739, 'you'), (1740, 'roll'), (1741, 'a'), (1742, '5'), (1743, 'on'), (1744, 'the'), (1745, 'outer'), (1746, 'double'), (1747, 'die'), (1748, ','), (1749, 'you'), (1750, 'have'), (1751, '5'), (1752, 'chance'), (1753, 'to'), (1754, 'roll'), (1755, 'Gumpi'), (1756, ','), (1757, 'the'), (1758, 'wolf'), (1759, ','), (1760, 'on'), (1761, 'the'), (1762, 'Familiar'), (1763, 'die'), (1764, '.'), (1765, 'if'), (1766, 'you'), (1767, 'fail'), (1768, ','), (1769, 'you'), (1770, 'must'), (1771, 'try'), (1772, 'again'), (1773, ','), (1774, 'but'), (1775, 'only'), (1776, 'if'), (1777, 'you'), (1778, 'have'), (1779, 'another'), (1780, 'Animal'), (1781, 'Magic'), (1782, 'Guide'), (1783, 'Book'), (1784, 'Card'), (1785, 'to'), (1786, 'use'), (1787, 'on'), (1788, 'your'), (1789, 'next'), (1790, 'turn'), (1791, '.'), (1792, 'if'), (1793, 'you'), (1794, 'do'), (1795, 'not'), (1796, 'have'), (1797, 'another'), (1798, 'Animal'), (1799, 'Magic'), (1800, 'Guide'), (1801, 'Book'), (1802, 'Card'), (1803, ','), (1804, 'you'), (1805, 'must'), (1806, 'leave'), (1807, 'the'), (1808, 'Spirit'), (1809, 'Chamber'), (1810, 'in'), (1811, 'search'), (1812, 'of'), (1813, 'another'), (1814, '.'), (1815, 'Wizard'), (1816, 'Staff'), (1817, ':'), (1818, 'in'), (1819, 'the'), (1820, 'Fire'), (1821, 'Spirit'), (1822, 'Chamber'), (1823, ','), (1824, 'you'), (1825, 'will'), (1826, 'summon'), (1827, 'the'), (1828, 'spirit'), (1829, 'Pranxtor'), (1830, 'to'), (1831, 'help'), (1832, 'you'), (1833, 'earn'), (1834, 'your'), (1835, 'Wizard'), (1836, 'staff'), (1837, '.'), (1838, 'you'), (1839, 'will'), (1840, 'blind'), (1841, 'draw'), (1842, 'a'), (1843, 'staff'), (1844, 'from'), (1845, 'the'), (1846, 'cup'), (1847, '.'), (1848, 'if'), (1849, 'you'), (1850, 'succeed'), (1851, 'in'), (1852, 'draw'), (1853, 'your'), (1854, 'colored'), (1855, 'staff'), (1856, ','), (1857, 'place'), (1858, 'the'), (1859, 'staff'), (1860, 'into'), (1861, 'the'), (1862, 'arm'), (1863, 'of'), (1864, 'your'), (1865, 'wizard'), (1866, 'piece'), (1867, '.'), (1868, 'if'), (1869, 'you'), (1870, 'draw'), (1871, 'a'), (1872, 'staff'), (1873, 'whose'), (1874, 'color'), (1875, 'do'), (1876, 'not'), (1877, 'match'), (1878, 'your'), (1879, 'wizard'), (1880, ','), (1881, 'or'), (1882, 'the'), (1883, 'neutral'), (1884, 'staff'), (1885, ','), (1886, 'you'), (1887, 'have'), (1888, 'fail'), (1889, '.'), (1890, 'place'), (1891, 'the'), (1892, 'staff'), (1893, 'back'), (1894, 'into'), (1895, 'Pranxtor'), (1896, \"'s\"), (1897, 'cup'), (1898, '.'), (1899, 'you'), (1900, 'may'), (1901, 'try'), (1902, 'again'), (1903, 'if'), (1904, 'you'), (1905, 'have'), (1906, 'another'), (1907, 'Athame'), (1908, 'Card'), (1909, 'to'), (1910, 'use'), (1911, 'on'), (1912, 'your'), (1913, 'next'), (1914, 'turn'), (1915, '.'), (1916, 'if'), (1917, 'you'), (1918, 'do'), (1919, 'not'), (1920, 'have'), (1921, 'another'), (1922, 'Athame'), (1923, 'Card'), (1924, ','), (1925, 'you'), (1926, 'must'), (1927, 'leave'), (1928, 'the'), (1929, 'Spirit'), (1930, 'Chamber'), (1931, 'in'), (1932, 'search'), (1933, 'of'), (1934, 'another'), (1935, '.'), (1936, 'Wizard'), (1937, 'Hat'), (1938, ':'), (1939, 'in'), (1940, 'the'), (1941, 'Air'), (1942, 'Spirit'), (1943, 'Chamber'), (1944, 'you'), (1945, 'will'), (1946, 'call'), (1947, 'upon'), (1948, 'the'), (1949, 'spirit'), (1950, 'Jaypes'), (1951, 'to'), (1952, 'help'), (1953, 'you'), (1954, 'locate'), (1955, 'a'), (1956, 'magical'), (1957, 'beast'), (1958, '.'), (1959, 'take'), (1960, 'the'), (1961, 'Dragon'), (1962, 'Medallion'), (1963, 'and'), (1964, 'flip'), (1965, 'Medallion'), (1966, 'like'), (1967, 'a'), (1968, 'coin'), (1969, '.'), (1970, 'if'), (1971, 'the'), (1972, 'dragon'), (1973, 'land'), (1974, 'face'), (1975, 'up'), (1976, ','), (1977, 'you'), (1978, 'have'), (1979, 'locate'), (1980, 'a'), (1981, 'dragon'), (1982, 'and'), (1983, 'have'), (1984, 'earn'), (1985, 'a'), (1986, 'hat'), (1987, '.'), (1988, 'place'), (1989, 'the'), (1990, 'appropriate'), (1991, 'hat'), (1992, 'on'), (1993, 'the'), (1994, 'head'), (1995, 'of'), (1996, 'your'), (1997, 'wizard'), (1998, 'piece'), (1999, '.'), (2000, 'if'), (2001, 'the'), (2002, 'egg'), (2003, 'land'), (2004, 'face'), (2005, 'up'), (2006, ','), (2007, 'you'), (2008, 'have'), (2009, 'fail'), (2010, 'this'), (2011, 'task'), (2012, 'and'), (2013, 'must'), (2014, 'try'), (2015, 'again'), (2016, ','), (2017, 'but'), (2018, 'only'), (2019, 'if'), (2020, 'you'), (2021, 'have'), (2022, 'another'), (2023, 'Dragon'), (2024, 'Medallion'), (2025, 'Card'), (2026, '.'), (2027, 'if'), (2028, 'you'), (2029, 'do'), (2030, 'not'), (2031, 'have'), (2032, 'another'), (2033, 'Dragon'), (2034, 'Medallion'), (2035, 'Card'), (2036, ','), (2037, 'you'), (2038, 'must'), (2039, 'leave'), (2040, 'the'), (2041, 'Spirit'), (2042, 'Chamber'), (2043, 'in'), (2044, 'search'), (2045, 'of'), (2046, 'another'), (2047, '.'), (2048, 'amulet'), (2049, ':'), (2050, 'in'), (2051, 'the'), (2052, 'Earth'), (2053, 'Spirit'), (2054, 'Chamber'), (2055, 'you'), (2056, 'will'), (2057, 'call'), (2058, 'upon'), (2059, 'the'), (2060, 'spirit'), (2061, 'Larfor'), (2062, 'to'), (2063, 'help'), (2064, 'you'), (2065, 'perform'), (2066, 'a'), (2067, 'special'), (2068, 'magic'), (2069, 'trick'), (2070, '.'), (2071, 'an'), (2072, 'oppose'), (2073, 'apprentice'), (2074, 'must'), (2075, 'hold'), (2076, 'the'), (2077, 'magic'), (2078, 'wand'), (2079, 'and'), (2080, 'place'), (2081, 'the'), (2082, 'first'), (2083, 'of'), (2084, '2'), (2085, 'identical'), (2086, 'levitate'), (2087, 'magnet'), (2088, 'onto'), (2089, 'wand'), (2090, '.'), (2091, 'you'), (2092, 'must'), (2093, 'then'), (2094, 'place'), (2095, 'the'), (2096, 'second'), (2097, 'magnet'), (2098, 'on'), (2099, 'the'), (2100, 'wand'), (2101, '-'), (2102, 'guess'), (2103, 'which'), (2104, 'way'), (2105, 'to'), (2106, 'put'), (2107, 'magnet'), (2108, '.'), (2109, 'if'), (2110, 'the'), (2111, 'magnet'), (2112, 'levitate'), (2113, ','), (2114, 'you'), (2115, 'have'), (2116, 'succeed'), (2117, 'and'), (2118, 'can'), (2119, 'collect'), (2120, 'your'), (2121, 'amulet'), (2122, '.'), (2123, 'if'), (2124, 'the'), (2125, 'magnet'), (2126, 'fall'), (2127, ','), (2128, 'you'), (2129, 'have'), (2130, 'fail'), (2131, 'and'), (2132, 'must'), (2133, 'try'), (2134, 'again'), (2135, ','), (2136, 'but'), (2137, 'only'), (2138, 'if'), (2139, 'you'), (2140, 'have'), (2141, 'another'), (2142, 'Ring'), (2143, 'of'), (2144, 'Power'), (2145, 'Card'), (2146, 'to'), (2147, 'use'), (2148, 'on'), (2149, 'your'), (2150, 'next'), (2151, 'turn'), (2152, '.'), (2153, 'if'), (2154, 'you'), (2155, 'do'), (2156, 'not'), (2157, 'have'), (2158, 'another'), (2159, 'Ring'), (2160, 'of'), (2161, 'PowerCard'), (2162, 'you'), (2163, 'must'), (2164, 'leave'), (2165, 'the'), (2166, 'Spirit'), (2167, 'Chamber'), (2168, 'in'), (2169, 'search'), (2170, 'of'), (2171, 'another'), (2172, '.'), (2173, 'magical'), (2174, 'duel'), (2175, ':'), (2176, 'if'), (2177, 'you'), (2178, 'land'), (2179, 'on'), (2180, 'the'), (2181, 'same'), (2182, 'space'), (2183, 'as'), (2184, 'another'), (2185, 'apprentice'), (2186, ','), (2187, 'you'), (2188, 'may'), (2189, 'challenge'), (2190, 'that'), (2191, 'player'), (2192, 'to'), (2193, 'a'), (2194, 'Magical'), (2195, 'Duel'), (2196, '.'), (2197, 'before'), (2198, 'duel'), (2199, 'begin'), (2200, ','), (2201, 'the'), (2202, 'challenging'), (2203, 'apprentice'), (2204, 'bet'), (2205, 'a'), (2206, 'certain'), (2207, 'number'), (2208, 'of'), (2209, 'card'), (2210, '.'), (2211, 'any'), (2212, 'amount'), (2213, 'and'), (2214, 'combination'), (2215, 'of'), (2216, 'card'), (2217, 'can'), (2218, 'be'), (2219, 'use'), (2220, ','), (2221, 'but'), (2222, 'you'), (2223, 'can'), (2224, 'not'), (2225, 'bet'), (2226, 'more'), (2227, 'card'), (2228, 'than'), (2229, 'the'), (2230, 'other'), (2231, 'player'), (2232, 'have'), (2233, '.'), (2234, 'each'), (2235, 'apprentice'), (2236, 'then'), (2237, 'take'), (2238, 'one'), (2239, 'set'), (2240, 'of'), (2241, 'Magical'), (2242, 'Duel'), (2243, 'Cards'), (2244, '.'), (2245, 'each'), (2246, 'set'), (2247, 'consist'), (2248, 'of'), (2249, '3'), (2250, 'attack'), (2251, 'card'), (2252, ':'), (2253, 'Spell'), (2254, ','), (2255, 'Potion'), (2256, ','), (2257, 'and'), (2258, 'Chant'), (2259, '.'), (2260, 'the'), (2261, 'duel'), (2262, 'player'), (2263, 'shuffle'), (2264, 'player'), (2265, 'card'), (2266, '.'), (2267, 'then'), (2268, ','), (2269, 'each'), (2270, 'player'), (2271, 'place'), (2272, 'the'), (2273, 'first'), (2274, '2'), (2275, 'card'), (2276, 'face'), (2277, 'down'), (2278, 'and'), (2279, 'flip'), (2280, 'the'), (2281, 'third'), (2282, 'card'), (2283, 'over'), (2284, 'to'), (2285, 'reveal'), (2286, 'the'), (2287, 'attack'), (2288, '.'), (2289, 'the'), (2290, 'card'), (2291, 'score'), (2292, 'as'), (2293, 'follow'), (2294, ':'), (2295, 'Spell'), (2296, 'beat'), (2297, 'Potion'), (2298, ','), (2299, 'potion'), (2300, 'beat'), (2301, 'Chant'), (2302, ','), (2303, 'and'), (2304, 'change'), (2305, 'beat'), (2306, 'Spell'), (2307, '.'), (2308, 'in'), (2309, 'the'), (2310, 'case'), (2311, 'of'), (2312, 'a'), (2313, 'tie'), (2314, '.'), (2315, 'apprentice'), (2316, 're'), (2317, '-'), (2318, 'shuffle'), (2319, 'apprentice'), (2320, 'card'), (2321, 'and'), (2322, 'start'), (2323, 'again'), (2324, 'until'), (2325, 'there'), (2326, 'be'), (2327, 'a'), (2328, 'winner'), (2329, '.'), (2330, 'the'), (2331, 'winner'), (2332, 'take'), (2333, 'as'), (2334, 'many'), (2335, 'card'), (2336, 'as'), (2337, 'be'), (2338, 'bet'), (2339, 'from'), (2340, 'the'), (2341, 'other'), (2342, 'player'), (2343, '.'), (2344, 'if'), (2345, 'you'), (2346, 'land'), (2347, 'on'), (2348, 'a'), (2349, 'Magical'), (2350, 'Duel'), (2351, 'space'), (2352, ','), (2353, 'move'), (2354, 'directly'), (2355, 'to'), (2356, 'another'), (2357, 'player'), (2358, \"'s\"), (2359, 'space'), (2360, 'to'), (2361, 'begin'), (2362, 'a'), (2363, 'duel'), (2364, '.'), (2365, 'the'), (2366, 'challenge'), (2367, 'player'), (2368, 'do'), (2369, 'not'), (2370, 'abide'), (2371, 'by'), (2372, 'the'), (2373, 'rule'), (2374, 'of'), (2375, 'the'), (2376, 'space'), (2377, 'player'), (2378, 'or'), (2379, 'player'), (2380, 'have'), (2381, 'move'), (2382, 'to'), (2383, '.'), (2384, 'win'), (2385, 'the'), (2386, 'GAME'), (2387, ':'), (2388, 'once'), (2389, 'you'), (2390, 'have'), (2391, 'complete'), (2392, 'all'), (2393, '4'), (2394, 'task'), (2395, 'and'), (2396, 'have'), (2397, 'in'), (2398, 'your'), (2399, 'possession'), (2400, 'the'), (2401, '4'), (2402, 'talisman'), (2403, ','), (2404, 'you'), (2405, 'be'), (2406, 'no'), (2407, 'long'), (2408, 'an'), (2409, 'apprentice'), (2410, ','), (2411, 'but'), (2412, 'be'), (2413, 'worthy'), (2414, 'of'), (2415, 'the'), (2416, 'title'), (2417, 'of'), (2418, '\"'), (2419, 'WizardYou'), (2420, 'may'), (2421, 'now'), (2422, 'proceed'), (2423, 'back'), (2424, 'to'), (2425, 'the'), (2426, 'center'), (2427, 'of'), (2428, 'the'), (2429, 'maze'), (2430, 'to'), (2431, 'perfrom'), (2432, 'the'), (2433, 'final'), (2434, 'test'), (2435, '.'), (2436, 'if'), (2437, 'you'), (2438, 'have'), (2439, 'a'), (2440, 'Magic'), (2441, 'Carpet'), (2442, 'Card'), (2443, ','), (2444, 'it'), (2445, 'would'), (2446, 'be'), (2447, 'wise'), (2448, 'to'), (2449, 'use'), (2450, 'Card'), (2451, 'now'), (2452, 'to'), (2453, 'fly'), (2454, 'directly'), (2455, 'to'), (2456, 'the'), (2457, 'center'), (2458, '.'), (2459, 'if'), (2460, 'not'), (2461, ','), (2462, 'you'), (2463, 'must'), (2464, 'travel'), (2465, 'back'), (2466, 'to'), (2467, 'the'), (2468, 'center'), (2469, 'through'), (2470, 'the'), (2471, 'maze'), (2472, '.'), (2473, 'this'), (2474, 'be'), (2475, 'dangerous'), (2476, 'because'), (2477, 'this'), (2478, 'leave'), (2479, 'you'), (2480, 'open'), (2481, 'to'), (2482, 'lose'), (2483, 'your'), (2484, 'talisman'), (2485, 'along'), (2486, 'the'), (2487, 'way'), (2488, '.'), (2489, 'if'), (2490, 'you'), (2491, 'make'), (2492, 'this'), (2493, 'to'), (2494, 'the'), (2495, 'center'), (2496, 'unscathed'), (2497, ','), (2498, 'you'), (2499, 'be'), (2500, 'safe'), (2501, '!'), (2502, '3'), (2503, '.'), (2504, 'in'), (2505, 'order'), (2506, 'to'), (2507, 'free'), (2508, 'Merlin'), (2509, ','), (2510, 'you'), (2511, 'must'), (2512, 'roll'), (2513, 'a'), (2514, 'combination'), (2515, 'of'), (2516, '7'), (2517, 'with'), (2518, 'the'), (2519, 'double'), (2520, 'die'), (2521, ':'), (2522, '4'), (2523, 'and'), (2524, '3'), (2525, ','), (2526, '5'), (2527, 'and'), (2528, '2'), (2529, 'or'), (2530, '6'), (2531, 'and'), (2532, '1'), (2533, '.'), (2534, 'you'), (2535, 'get'), (2536, 'only'), (2537, 'one'), (2538, 'chance'), (2539, 'per'), (2540, 'turn'), (2541, '.'), (2542, 'if'), (2543, 'you'), (2544, 'successfully'), (2545, 'roll'), (2546, 'a'), (2547, '7'), (2548, ','), (2549, 'you'), (2550, 'have'), (2551, 'free'), (2552, 'Merlin'), (2553, 'and'), (2554, 'win'), (2555, 'the'), (2556, 'game'), (2557, '!'), (2558, '5'), (2559, '.'), (2560, 'beware'), (2561, '!'), (2562, 'if'), (2563, 'you'), (2564, 'roll'), (2565, 'a'), (2566, 'combination'), (2567, 'of'), (2568, '3'), (2569, '('), (2570, '2'), (2571, 'and'), (2572, '1'), (2573, ')'), (2574, ','), (2575, 'the'), (2576, '\"'), (2577, 'Rule'), (2578, 'of'), (2579, 'Three'), (2580, '\"'), (2581, 'apply'), (2582, 'and'), (2583, 'you'), (2584, 'will'), (2585, 'be'), (2586, 'send'), (2587, 'back'), (2588, 'to'), (2589, 'the'), (2590, 'last'), (2591, 'Spirit'), (2592, 'Chamber'), (2593, 'you'), (2594, 'visit'), (2595, '.'), (2596, 'you'), (2597, 'must'), (2598, 'travel'), (2599, 'back'), (2600, 'to'), (2601, 'the'), (2602, 'center'), (2603, 'to'), (2604, 'try'), (2605, 'again'), (2606, '!')]\n",
            "2607 12624\n",
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''the following cat is dead. the truck's white area.'''\n",
        "\n",
        "# text = get_document_by_line(CLEANED_DATASET_FILE_PATH, 130)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text.lower())\n",
        "print([(token.lemma_, token.head, token.dep_, token.pos_) for token in doc])\n",
        "print(len(doc), len(doc.text))\n",
        "\n",
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "id": "19Y_u25zdCnM",
        "outputId": "d982d4db-141e-413d-946e-014b2174b40e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "id": "19Y_u25zdCnM",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', cat, 'det', 'DET'), ('follow', cat, 'amod', 'VERB'), ('cat', is, 'nsubj', 'NOUN'), ('be', is, 'ROOT', 'AUX'), ('dead', is, 'acomp', 'ADJ'), ('.', is, 'punct', 'PUNCT'), ('the', truck, 'det', 'DET'), ('truck', area, 'poss', 'NOUN'), (\"'s\", truck, 'case', 'PART'), ('white', area, 'amod', 'ADJ'), ('area', area, 'ROOT', 'NOUN'), ('.', area, 'punct', 'PUNCT')]\n",
            "12 50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6da7b40c00e5429b91778b913094bdc4-0\" class=\"displacy\" width=\"1800\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">following</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">cat</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">dead.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">truck</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">'s</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">white</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">area.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-6da7b40c00e5429b91778b913094bdc4-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-6da7b40c00e5429b91778b913094bdc4-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-6da7b40c00e5429b91778b913094bdc4-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-6da7b40c00e5429b91778b913094bdc4-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-6da7b40c00e5429b91778b913094bdc4-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-6da7b40c00e5429b91778b913094bdc4-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-6da7b40c00e5429b91778b913094bdc4-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-6da7b40c00e5429b91778b913094bdc4-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-6da7b40c00e5429b91778b913094bdc4-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-6da7b40c00e5429b91778b913094bdc4-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-6da7b40c00e5429b91778b913094bdc4-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,2.0 1625.0,2.0 1625.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-6da7b40c00e5429b91778b913094bdc4-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-6da7b40c00e5429b91778b913094bdc4-0-6\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-6da7b40c00e5429b91778b913094bdc4-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1270.0,179.0 L1278.0,167.0 1262.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-6da7b40c00e5429b91778b913094bdc4-0-7\" stroke-width=\"2px\" d=\"M1470,177.0 C1470,89.5 1620.0,89.5 1620.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-6da7b40c00e5429b91778b913094bdc4-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470,179.0 L1462,167.0 1478,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from multi_rake import Rake\n",
        "# from summa import keywords\n",
        "import yake\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = clean_text(get_document_by_line(CLEANED_DATASET_FILE_PATH, 155))\n",
        "\n",
        "# def use_rake(text: str):\n",
        "#     rake = Rake()\n",
        "#     keywords = rake.apply(text)\n",
        "#     return keywords[:30]\n",
        "\n",
        "def use_yake(text: str):\n",
        "    kw_extractor = yake.KeywordExtractor(top=20)\n",
        "    keywords_info = kw_extractor.extract_keywords(text)\n",
        "    keyword_groups = [keyword_info[0] for keyword_info in keywords_info if keyword_info[1] < 0.1]\n",
        "    return keyword_groups\n",
        "\n",
        "# def use_TextRank(text: str):\n",
        "#     TR_keywords = keywords.keywords(text, scores=True)\n",
        "#     return TR_keywords\n",
        "\n",
        "# display(use_rake(text))\n",
        "# display(use_TextRank(text))\n",
        "use_yake(text)"
      ],
      "metadata": {
        "id": "MAU5oGis58L5"
      },
      "id": "MAU5oGis58L5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.language import Language\n",
        "\n",
        "def _component_assign_sentence_id(doc: spacy.tokens.Doc) -> spacy.tokens.Doc:\n",
        "    spacy.tokens.Token.set_extension('sentence_id', default=None, force=True)\n",
        "    cur_sentence_id = -1\n",
        "    for token in doc:\n",
        "        if token.is_sent_start:\n",
        "            cur_sentence_id += 1\n",
        "        token._.sentence_id = cur_sentence_id\n",
        "        \n",
        "    return doc\n",
        "\n",
        "factory_id = 'assign_sentence_id'\n",
        "if not Language.has_factory(factory_id):\n",
        "    @Language.component(factory_id)\n",
        "    def assign_sentence_id(doc: spacy.tokens.Doc) -> spacy.tokens.Doc:\n",
        "        return _component_assign_sentence_id(doc)\n",
        "\n",
        "text = \"\"\"this is the first sentence. this is the second one. the third one is here.\"\"\"\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('assign_sentence_id', after='parser')\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token, token._.sentence_id)"
      ],
      "metadata": {
        "id": "Uq749ZgDE1Ae",
        "outputId": "aad4056f-f6b4-43e0-885c-e0eb3ea9bbd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Uq749ZgDE1Ae",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this 0\n",
            "is 0\n",
            "the 0\n",
            "first 0\n",
            "sentence 0\n",
            ". 0\n",
            "this 1\n",
            "is 1\n",
            "the 1\n",
            "second 1\n",
            "one 1\n",
            ". 1\n",
            "the 2\n",
            "third 2\n",
            "one 2\n",
            "is 2\n",
            "here 2\n",
            ". 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "from typing import List, Set, Dict\n",
        "from string import punctuation\n",
        "\n",
        "MIN_TOKEN_TO_BE_CONSIDERED_UNIGRAM = 4\n",
        "MIN_TOKEN_TO_BE_CONSIDERED_BIGRAM = 2\n",
        "IGNORED_WORDS = {\n",
        "        'amount', 'beginning', 'board', 'book', 'bottom', 'case', 'choice', \n",
        "        'clarification', 'clockwise', 'condition', 'cost', 'design', 'effect', \n",
        "        'end', 'example', 'face', 'front', 'game', 'left', 'middle', 'note', 'number', \n",
        "        'opponent', 'option', 'order', 'overview', 'page', 'play',\n",
        "        'purpose', 'reference', 'result', 'right', 'rule', 'rulebook', \n",
        "        'section', 'set', 'setup', 'side', 'summary', 'start', 'step', 'table', 'thing',\n",
        "        'type', 'tie', 'time', 'top', 'total', 'use', 'value', 'version', 'way'\n",
        "        }.union(spacy.load('en_core_web_sm').Defaults.stop_words)\n",
        "\n",
        "def find_most_common_nouns(doc: spacy.tokens.Doc) -> Dict[str, List[spacy.tokens.Token]]:\n",
        "    tokens_dict = defaultdict(list)\n",
        "\n",
        "    for token in doc:\n",
        "        if len(token) >= 3 and \\\n",
        "            token.pos_ in {'NOUN', 'PROPN'} and \\\n",
        "            token.dep_ in {'nsubj', 'dobj', 'nsubjpass', 'pobj'}:\n",
        "            tokens_dict[token.lemma_.lower()].append(token)\n",
        "           \n",
        "    return tokens_dict\n",
        "\n",
        "def _is_token_part_of_bigram(token: spacy.tokens.Token, \n",
        "                             unigram_token: spacy.tokens.Token) -> bool:\n",
        "    return token.dep_ == 'compound' and \\\n",
        "        token.pos_ in {'NOUN', 'PROPN'} and \\\n",
        "        not token.text.endswith(tuple(punctuation)) and \\\n",
        "        not token.text.startswith(tuple(punctuation)) and \\\n",
        "        token.head.i == unigram_token.i\n",
        "\n",
        "def find_most_relevant_ngram(doc: spacy.tokens.Doc,\n",
        "                             unigrams: Dict[str, List[spacy.tokens.Token]]) \\\n",
        "                             -> Dict[str, Set[str]]:\n",
        "    # TODO: wildcard * to mark that there are unigrams lead by a non-relevant word\n",
        "    excluded_bigrams = IGNORED_WORDS.union(set(unigrams.keys()))\n",
        "    bigram_associated_dict = defaultdict(Counter)\n",
        "    for name, tokens in unigrams.items():\n",
        "        for token in tokens:\n",
        "            possible_bigram = doc[token.i - 1]\n",
        "            if token.i > 0 and _is_token_part_of_bigram(possible_bigram, token) and \\\n",
        "                possible_bigram.lemma_ not in excluded_bigrams:\n",
        "                bigram_associated_dict[name][possible_bigram.lemma_] += 1\n",
        "\n",
        "    return defaultdict(set, { \n",
        "        unigram: set(bigram for bigram, counter in bigrams.items() \n",
        "                     if counter >= MIN_TOKEN_TO_BE_CONSIDERED_BIGRAM)\n",
        "        for unigram, bigrams in bigram_associated_dict.items() \n",
        "    })\n",
        "\n",
        "\n",
        "def find_most_relevant_unigrams(doc: spacy.tokens.Doc) \\\n",
        "                                -> Dict[str, List[spacy.tokens.Token]]:\n",
        "    # TODO: should at least 2 subsequent sentences contain the same unigram?\n",
        "    possible_components_info = dict(filter(lambda token: token[0] not in IGNORED_WORDS and \n",
        "                                           len(token[1]) >= MIN_TOKEN_TO_BE_CONSIDERED_UNIGRAM and\n",
        "                                           any(token_occurrence.dep_ in {'nsubj', 'nsubjpass', 'dobj'}\n",
        "                                               for token_occurrence in token[1]), \n",
        "                                           find_most_common_nouns(doc).items()))\n",
        "\n",
        "    return possible_components_info\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = get_document_by_line(CLEANED_DATASET_FILE_PATH, 232)\n",
        "display(text)\n",
        "doc = nlp(text.lower())\n",
        "unigrams = find_most_relevant_unigrams(doc)\n",
        "display(unigrams)\n",
        "ngrams = find_most_relevant_ngram(doc, unigrams)\n",
        "display(ngrams)\n",
        "\n",
        "'''\n",
        "defaultdict(set,\n",
        "            {'abc': {'inc'},\n",
        "             'tile': {'building', 'edition', 'expansion', 'lake'},\n",
        "             'space': {'stack'},\n",
        "             'border': {'lake'},\n",
        "             'market': {'estate'},\n",
        "             'marker': {'investment'}})\n",
        "             '''"
      ],
      "metadata": {
        "id": "_t7h6TLUZoef"
      },
      "id": "_t7h6TLUZoef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Set, Dict\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "def find_interactions_score(doc: spacy.tokens.Doc, \n",
        "                             unigrams: Dict[str, List[spacy.tokens.Token]],\n",
        "                             unigrams_with_weight: Dict[str, int]) -> float:\n",
        "    matrix_len = len(unigrams)\n",
        "    matrix = [[0 for c in range(matrix_len)] for r in range(matrix_len)]\n",
        "    unigrams_product = product(enumerate(unigrams.items()), enumerate(unigrams.items()))\n",
        "    for (ir, (ug1, tokens1)), (ic, (ug2, tokens2)) in unigrams_product:\n",
        "        # I only fill half of the matrix, the other half is symmetrical to the first\n",
        "        # one. The main diagonal is useless because there is no interaction between\n",
        "        # a component and itself, by definition\n",
        "        if ir >= ic:\n",
        "            continue\n",
        "        matrix[ir][ic] = len(set(token._.sentence_id for token in tokens1) \\\n",
        "            .union(set(token._.sentence_id for token in tokens2)))\n",
        "    \n",
        "    # copy the values of the filled half of the matrix to the skipped half\n",
        "    # for ir in range(matrix_len):\n",
        "    #     for ic in range(0, ir):\n",
        "    #         matrix[ir][ic] = matrix[ic][ir]\n",
        "\n",
        "    # TODO: evaluate if an interaction need to be considered as 2. normalize results based on weight\n",
        "    # maybe useful sentences is a good feature?\n",
        "    interaction_count = sum(sum(_ for _ in row) for row in matrix)\n",
        "\n",
        "    return interaction_count / len(unigrams)\n",
        "\n",
        "def find_most_relevant_sentences(doc: spacy.tokens.Doc, relevant_unigrams: Set[str]) \\\n",
        "        -> List[spacy.tokens.Span]:\n",
        "    return [sent for sent in doc.sents if any(token.pos_ == 'VERB' for token in sent) and \\\n",
        "        any(token.lemma_ in relevant_unigrams for token in sent)]\n",
        "\n",
        "def get_bg_components(text: str) -> Dict[str, List[int]]:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    nlp.add_pipe('assign_sentence_id', after='parser')\n",
        "    original_text = text\n",
        "    doc = nlp(text.lower())\n",
        "    most_relevant_unigrams = find_most_relevant_unigrams(doc)\n",
        "    unigrams_keys = set(most_relevant_unigrams.keys())\n",
        "    most_relevant_ngrams = find_most_relevant_ngram(doc, most_relevant_unigrams)\n",
        "    unigrams_with_weight = { unigram: max(1, len(most_relevant_ngrams[unigram])) \n",
        "                             for unigram in unigrams_keys }\n",
        "    most_relevant_sentences = find_most_relevant_sentences(doc, unigrams_keys)\n",
        "    interactions_score = find_interactions_score(doc, most_relevant_unigrams, unigrams_with_weight)\n",
        "    print(\"interaction score: \", interactions_score)\n",
        "    display(most_relevant_ngrams)\n",
        "    display(len([sent for sent in doc.sents]), len(most_relevant_sentences), most_relevant_sentences)\n",
        "\n",
        "    return most_relevant_unigrams\n",
        "\n",
        "def get_doc_variance(doc: spacy.tokens.Doc, components_dict: Dict[str, List[int]]) -> float:\n",
        "    '''variance measures how components interleave in the text. This could mean that rules involve\n",
        "    many components and are therefore more complex. variancy is computed using `np.var` on each\n",
        "    component list. the results are normalized by multiplicating for the frequency of the component.\n",
        "    eventually the partial variances are summed together and the result normalized with the \n",
        "    total numbers of tokens.'''\n",
        "    tokens_count = sum(len(token_list) for token_list in components_dict.values())\n",
        "    return sum((len(tokens) / tokens_count) * np.var([token.i for token in tokens])\n",
        "        for tokens in components_dict.values()) / len((doc))\n",
        "\n",
        "\n",
        "text = get_document_by_line(CLEANED_DATASET_FILE_PATH, 198)\n",
        "display(text)\n",
        "components = get_bg_components(text)\n",
        "display(components.keys())\n",
        "print(len(components.keys()))"
      ],
      "metadata": {
        "id": "TKh5Jf-xovbn"
      },
      "id": "TKh5Jf-xovbn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "print(nlp.Defaults.stop_words)\n",
        "doc = nlp(get_document_by_line(CLEANED_DATASET_FILE_PATH, 130))\n",
        "for sent in doc.sents:\n",
        "    print(sent)"
      ],
      "metadata": {
        "id": "xYRZ-I34QJRO"
      },
      "id": "xYRZ-I34QJRO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "\n",
        "def find_n_most_common_nouns(n, docs: List[spacy.tokens.Doc]) -> List[str]:\n",
        "    docs_sets = [set(find_most_common_nouns(doc).keys())\n",
        "                 for doc in docs]\n",
        "    all_tokens_from_docs = itertools.chain(*docs_sets)\n",
        "    tokens_counter = Counter(all_tokens_from_docs)\n",
        "    return tokens_counter.most_common(n)\n",
        "    \n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "df_dataset = get_df_with_docs(CLEANED_DATASET_FILE_PATH, 100, 200)\n",
        "docs = nlp.pipe(df_dataset['rulebook'].values)\n",
        "\n",
        "find_n_most_common_nouns(80, docs)"
      ],
      "metadata": {
        "id": "w-JVFrfg4BwA"
      },
      "id": "w-JVFrfg4BwA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d6596cd-3fd1-4e50-9dce-e6c333667182",
      "metadata": {
        "id": "7d6596cd-3fd1-4e50-9dce-e6c333667182"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import pandas as pd\n",
        "import ast\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def get_rules_features(id: int, doc: spacy.tokens.Doc) -> Tuple[int, float]:\n",
        "    logger.info(f'processing board game {id}')\n",
        "    rulebook_len = len(doc)\n",
        "    bg_components = get_bg_components(doc)\n",
        "    print(bg_components)\n",
        "\n",
        "    return 0, 0\n",
        "    # rules = get_rules(text)\n",
        "    # rule_count = len(rules)\n",
        "    # return rule_count, len(text) / rule_count\n",
        "\n",
        "def apply_for_rulebook_features(row, docs_dict):\n",
        "    next_doc_info = next(docs_dict)\n",
        "    assert next_doc_info[0] == row.id\n",
        "    return pd.Series(get_rules_features(row.id, next_doc_info[1]), \n",
        "                     index=['rule_count', 'avg_rule_len'])\n",
        "\n",
        "PROCESSED_DATASET_FILE_PATH = 'data/processed_dataset.csv' if WORKING_LOCALLY \\\n",
        "    else '/content/drive/My Drive/Projects/IRBoardGameComplexity/processed_dataset.csv'\n",
        "\n",
        "# ast.literal_eval converts the family column string into a python array\n",
        "df_dataset = pd.read_csv(CLEANED_DATASET_FILE_PATH, converters={ 'family': ast.literal_eval }, nrows=1)\n",
        "remove_columns_prefix(df_dataset)\n",
        "docs_dict = zip(df_dataset['id'].values, \n",
        "                nlp.pipe(map(clean_text, df_dataset['rulebook'].values)))\n",
        "\n",
        "df_rules_features = df_dataset.apply(lambda x: apply_for_rulebook_features(x, docs_dict),\n",
        "                                     axis='columns')\n",
        "df_features = df_dataset[['averageweight', 'playingtime', 'family']].join(df_rules_features)\n",
        "        \n",
        "# one-hot encoding \"family\" field \n",
        "# from https://stackoverflow.com/questions/71401193/one-hot-encoding-in-python-for-array-values-in-a-dataframe\n",
        "df_features = df_features.join(df_features.pop('family').apply('|'.join).str.get_dummies())\n",
        "df_features.head()\n",
        "\n",
        "# df_features.to_csv(PROCESSED_DATASET_FILE_PATH, header=True, index=False, mode='w')    \n",
        "# if not WORKING_LOCALLY:\n",
        "#     drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}