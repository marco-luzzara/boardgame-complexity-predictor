{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-luzzara/boardgame-complexity-predictor/blob/master/src/extract_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "gOE9-ZKtwDuH",
      "metadata": {
        "id": "gOE9-ZKtwDuH"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import os\n",
        "WORKING_LOCALLY = bool(os.getenv('WORKING_LOCALLY'))\n",
        "\n",
        "if WORKING_LOCALLY:\n",
        "    DATASET_FILE_PATH = 'data/dataset.csv'\n",
        "    CLEANED_DATASET_FILE_PATH = 'data/cleaned_dataset.csv'\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATASET_FILE_PATH = '/content/drive/My Drive/Projects/IRBoardGameComplexity/dataset.csv'\n",
        "    CLEANED_DATASET_FILE_PATH = '/content/drive/My Drive/Projects/IRBoardGameComplexity/cleaned_dataset.csv'\n",
        "    !pip install spacy-transformers\n",
        "    !python3 -m pip install coreferee==1.3.*\n",
        "    !python3 -m coreferee install en\n",
        "    !python -m spacy download en_core_web_lg\n",
        "    !python -m spacy download en_core_web_trf\n",
        "    !pip install git+https://github.com/LIAAD/yake\n",
        "    !pip install rake-nltk\n",
        "    clear_output(wait=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3aa4a086-aa20-4def-b17a-3b4ff4fad93f",
      "metadata": {
        "id": "3aa4a086-aa20-4def-b17a-3b4ff4fad93f"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "## +++++++++++ with coreferee\n",
        "import coreferee\n",
        "nlp = spacy.load('en_core_web_trf')\n",
        "nlp.add_pipe(\"coreferee\")\n",
        "\n",
        "clear_output(wait=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a4bcea26-c1a3-45fb-845e-98cae3073e85",
      "metadata": {
        "id": "a4bcea26-c1a3-45fb-845e-98cae3073e85",
        "outputId": "a2de84ff-5cf2-46af-ed46-89db8647a127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-11-24 21:03:24,109 bgg_predict  DEBUG    test\n",
            "DEBUG:bgg_predict:test\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger('bgg_predict')\n",
        "logger.handlers.clear()\n",
        "handler = logging.StreamHandler()\n",
        "formatter = logging.Formatter(\n",
        "        '%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "logger.addHandler(handler)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "logger.debug('test')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "regex_mail = re.compile(r'\\w+(?:\\.\\w+)*?@\\w+(?:\\.\\w+)+')\n",
        "# modified from https://stackoverflow.com/a/163684/5587393\n",
        "regex_link = re.compile(r'(?:\\b(?:(?:https?|ftp|file)://|www))[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#%=~_|]')\n",
        "# in a sentence there must be at least 4 words of length 2 each\n",
        "regex_at_least_4_words_in_sentence = re.compile(r\"^(?=.*?(?:[,:;()'\\\"]?[a-zA-Z']{2,}[,:;()'\\\"]?(?: |-|$)(?:[^a-zA-Z]*?|[a-zA-Z]? ?)){4,})\")         \n",
        "# a string like \"first.Second\" could be misinterpreted by the tokenizer as a single token\n",
        "# with the regex it becomes \"first. Second\"\n",
        "regex_distance_between_period_and_following_word = re.compile(r'\\.(?!\\s|$)')\n",
        "# compress consecutive whitespaces\n",
        "regex_multiple_spaces = re.compile(r'\\s{2,}')\n",
        "# interrupted words usually have a \"- \" at the end before the new line, 'inter- rupted' -> 'interrupted'\n",
        "# NOTE: must be after whitespace compression\n",
        "regex_interrupted_word = re.compile(r'([a-zA-Z])- ')\n",
        "# remove page numbers, that are usually enclosed in characters like = or -, for example \"-12-\"\n",
        "regex_consecutive_meaningless_chars = re.compile(r'[^\\.a-zA-Z0-9\\s()]{2,} *(?:\\d+)?|(?P<prepage>[^a-zA-Z\\s\\d\\.])\\d+(?P=prepage)')\n",
        "# remove paragraphs id, '1.2.3' -> ''\n",
        "regex_dot_separated_digits = re.compile(r'(?:\\d+\\.)+\\d+')\n",
        "# remove meaningless chars after sentence start, '. (- start' -> '. start'\n",
        "regex_clean_start = re.compile(r'\\.(\\s?)[^a-zA-Z\\s]+')\n",
        "# recover missing apices\n",
        "regex_missing_apices = re.compile(r\"\\b([a-zA-Z]+) (t|s)\\b\")\n",
        "\n",
        "def clean_from_short_sentences(text: str) -> str:\n",
        "    return '.'.join(sentence for sentence in text.split('.') if regex_at_least_4_words_in_sentence.match(sentence) is not None)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    for clean_function in [lambda x: regex_mail.sub('', x),\n",
        "                           lambda x: regex_link.sub('', x),\n",
        "                           lambda x: regex_dot_separated_digits.sub('', x),\n",
        "                           lambda x: regex_consecutive_meaningless_chars.sub('', x),\n",
        "                           lambda x: regex_clean_start.sub(r'.\\1', x),\n",
        "                           # everything that is remove should be placed before this line so that \n",
        "                           # eventual spaces are compressed with regex_multiple_space\n",
        "                           lambda x: regex_multiple_spaces.sub(' ', x),\n",
        "                           lambda x: regex_interrupted_word.sub(r'\\1', x),\n",
        "                           lambda x: regex_missing_apices.sub(r\"\\1'\\2\", x),\n",
        "                           lambda x: clean_from_short_sentences(x),\n",
        "                           lambda x: regex_distance_between_period_and_following_word.sub('. ', x)]:\n",
        "        text = clean_function(text)\n",
        "    return text\n",
        "\n",
        "test_text = 'this is a test (me@gmail.it) -12- that wi-  ll be   cleaned. with 2 5 6 not valid. two sentences can t be good http://or.not.'\n",
        "cleaned_text = clean_text(test_text)\n",
        "print(cleaned_text)\n",
        "assert cleaned_text == 'this is a test () that will be cleaned. two sentences can\\'t be good '"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBHkUGAsukWJ",
        "outputId": "fd7cafc0-4c17-4aee-dec4-cb717331d626",
        "collapsed": true
      },
      "id": "bBHkUGAsukWJ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a test () that will be cleaned. two sentences can't be good \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import pandas as pd\n",
        "import coreferee\n",
        "\n",
        "def remove_columns_prefix(df: pd.DataFrame) -> None:\n",
        "    '''remove prefix 'info.' from the columns of df'''\n",
        "    df.rename(columns=lambda c: c.rsplit('.', 1)[-1], inplace=True)\n",
        "\n",
        "def _get_new_token_from_resolve(token: spacy.tokens.Token, \n",
        "                                chains: coreferee.data_model.ChainHolder) -> spacy.tokens.Token:\n",
        "    resolved_token = chains.resolve(token)\n",
        "    return token.text_with_ws if resolved_token is None \\\n",
        "                              else 'and '.join([res_token.text_with_ws + ' ' for res_token in resolved_token])   \n",
        "\n",
        "def _process_doc_for_coref(doc: spacy.tokens.Doc) -> str:\n",
        "    replacement_tokens = []\n",
        "    chains = doc._.coref_chains\n",
        "    new_doc_tokens_text = [_get_new_token_from_resolve(token, chains) for token in doc]\n",
        "\n",
        "    return ''.join(new_doc_tokens_text)\n",
        "\n",
        "def preprocess_texts(texts: List[str]) -> List[str]:\n",
        "    nlp = spacy.load('en_core_web_trf')\n",
        "    nlp.add_pipe(\"coreferee\")\n",
        "\n",
        "    texts = [clean_text(text) for text in texts]\n",
        "    docs = nlp.pipe(texts)\n",
        "\n",
        "    return [regex_multiple_spaces.sub(' ', _process_doc_for_coref(doc)) for doc in docs]\n",
        "        \n",
        "text = get_document_by_line(DATASET_FILE_PATH, 103)\n",
        "# text = '''Although he was very busy with his work, the magical Peter had had enough of it. \n",
        "#     He and his wife decided they needed a holiday. \n",
        "#     this couple travelled to Spain because it loves the country very much.'''\n",
        "preprocess_texts([text])"
      ],
      "metadata": {
        "id": "jpT4BvSfFWTd"
      },
      "id": "jpT4BvSfFWTd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import Generator, Tuple\n",
        "import ast\n",
        "\n",
        "SKIP_ROWS = 0\n",
        "CHUNK_SIZE = 20\n",
        "DATASET_ROWS = 381\n",
        "\n",
        "assert SKIP_ROWS > 0 # make sure you do not start preprocessing again\n",
        "\n",
        "def clean_data_row(row, docs_dict: Generator[Tuple[int, str], None, None]):\n",
        "    id_rulebook = next(docs_dict)\n",
        "    assert id_rulebook[0] == row['id']\n",
        "    row['rulebook'] = id_rulebook[1]\n",
        "    return row\n",
        "\n",
        "for skip_rows in range(SKIP_ROWS, DATASET_ROWS, CHUNK_SIZE):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    column_names = ['rulebook', 'info.id', 'info.name', 'info.averageweight', 'info.playingtime', 'info.family']\n",
        "    # ast.literal_eval converts the family column string into a python array\n",
        "    df_dataset = pd.read_csv(DATASET_FILE_PATH, converters={ 'info.family': ast.literal_eval },\n",
        "                            names=column_names, header=None,\n",
        "                            nrows=CHUNK_SIZE, skiprows=skip_rows)\n",
        "    remove_columns_prefix(df_dataset)\n",
        "    logger.info(f\"processing boardgames from {df_dataset.loc[0, 'id']} to {df_dataset.loc[df_dataset.index[-1], 'id']}\")\n",
        "    docs_dict = zip(df_dataset['id'].values, preprocess_texts(df_dataset['rulebook'].values))\n",
        "\n",
        "    df_cleaned_dataset = df_dataset.apply(lambda x: clean_data_row(x, docs_dict),\n",
        "                                        axis='columns')\n",
        "\n",
        "    df_cleaned_dataset.to_csv(CLEANED_DATASET_FILE_PATH, \n",
        "                            header=True if SKIP_ROWS == 1 else False, index=False, \n",
        "                            mode='w' if SKIP_ROWS == 1 else 'a')\n",
        "    if not WORKING_LOCALLY:\n",
        "        drive.flush_and_unmount()\n",
        "\n",
        "df_cleaned_dataset.head()"
      ],
      "metadata": {
        "id": "OfVY9Q2-TU02"
      },
      "id": "OfVY9Q2-TU02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "def get_df_with_docs(file_path: str, nrows=None, skiprows=1) -> pd.DataFrame:\n",
        "    ''' get a dataframe containing nrows and skipping the first `skiprows` (including the header)'''\n",
        "    df_dataset = pd.read_csv(file_path, converters={ 'family': ast.literal_eval }, \n",
        "                             nrows=nrows, skiprows=range(1, skiprows))\n",
        "    remove_columns_prefix(df_dataset)\n",
        "    return df_dataset\n",
        "\n",
        "def get_document_by_line(file_path: str, line: int) -> str:\n",
        "    ''' the line includes the header too '''\n",
        "    # range from 1 is used to keep the first row https://stackoverflow.com/a/27325729/5587393\n",
        "    df = get_df_with_docs(file_path, 1, line - 1)\n",
        "    return df['rulebook'].iloc[0]\n",
        "\n",
        "def get_document_by_id(file_path: str, id: int) -> str:\n",
        "     with pd.read_csv(file_path, chunksize=1, converters={ 'family': ast.literal_eval }) as reader:\n",
        "        while True:\n",
        "            df = next(reader)\n",
        "            bg_id = df['id'].iloc[0]\n",
        "            if bg_id == id:\n",
        "                return df['rulebook'].iloc[0]\n",
        "\n",
        "assert get_document_by_id(CLEANED_DATASET_FILE_PATH, 2310) == get_document_by_line(CLEANED_DATASET_FILE_PATH, 40)"
      ],
      "metadata": {
        "id": "VepMko8FPiyw"
      },
      "id": "VepMko8FPiyw",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "from collections import namedtuple\n",
        "from spacy.matcher import Matcher, DependencyMatcher\n",
        "\n",
        "LuckMetrics = namedtuple('LuckMetrics', ['dice_based', 'drawing_based', 'shuffling_based', 'random_based'])\n",
        "\n",
        "def get_luck_metrics(doc: spacy.tokens.Doc) -> LuckMetrics:\n",
        "    # ---------- random ----------\n",
        "    random_matcher = Matcher(doc.vocab)\n",
        "    random_patterns_match = [\n",
        "        [{\"LEMMA\": { \"IN\": [\"random\", \"randomly\"]}}]\n",
        "    ]\n",
        "    random_matcher.add(\"random\", random_patterns_match)\n",
        "\n",
        "    # ---------- shuffle ----------\n",
        "    shuffle_matcher = Matcher(doc.vocab)\n",
        "    shuffle_patterns_match = [\n",
        "        [{\"LEMMA\": \"shuffle\", \"POS\": \"VERB\"}]\n",
        "    ]\n",
        "    shuffle_matcher.add(\"shuffle\", shuffle_patterns_match)\n",
        "\n",
        "    # ---------- card drawing ----------\n",
        "    drawing_matcher = DependencyMatcher(doc.vocab)    \n",
        "    drawing_patterns = [\n",
        "        [\n",
        "            {\n",
        "                \"RIGHT_ID\": \"drawing\",\n",
        "                \"RIGHT_ATTRS\": {\"LEMMA\": \"draw\", \"POS\": \"VERB\"}\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"drawing\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"card\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": \"card\",\n",
        "                    \"POS\": \"NOUN\", \n",
        "                    \"DEP\": { \"IN\": ['dobj', 'nsubjpass', 'compound'] }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "    drawing_matcher.add(\"drawing\", drawing_patterns)\n",
        "    # ---------- dice rolling ----------\n",
        "    dice_matcher = DependencyMatcher(doc.vocab)    \n",
        "    dice_patterns = [\n",
        "        [\n",
        "            {\n",
        "                \"RIGHT_ID\": \"rolling\",\n",
        "                \"RIGHT_ATTRS\": {\"LEMMA\": { \"IN\": [\"use\", \"throw\", \"roll\"]}, \"POS\": \"VERB\"}\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"rolling\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"dice_or_die\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"die\", \"dice\"]},\n",
        "                    \"POS\": \"NOUN\", \n",
        "                    \"DEP\": { \"IN\": ['nsubj', 'dobj', 'nsubjpass', 'compound'] }\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"RIGHT_ID\": \"rolling\",\n",
        "                \"RIGHT_ATTRS\": {\"LEMMA\": { \"IN\": [\"use\", \"throw\", \"roll\"]}, \"POS\": \"VERB\"}\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"rolling\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"number\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"IS_DIGIT\": True, \n",
        "                    \"DEP\": { \"IN\": ['dobj'] }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "    dice_matcher.add(\"diceroll\", dice_patterns)\n",
        "\n",
        "    dice_matches = dice_matcher(doc) \n",
        "    draw_matches = drawing_matcher(doc)\n",
        "    shuffle_matches = shuffle_matcher(doc)\n",
        "    random_matches = random_matcher(doc)\n",
        "\n",
        "    # TODO: needs normalization? (divide by rulebook length or tokens)\n",
        "\n",
        "    return LuckMetrics(len(dice_matches), len(draw_matches), len(shuffle_matches), len(random_matches))\n",
        "\n",
        "text = get_document_by_line(CLEANED_DATASET_FILE_PATH, 130)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(clean_text(text))\n",
        "print(len(doc), len(doc.text))\n",
        "print(get_luck_metrics(doc))\n",
        "\n",
        "# displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "id": "Ayy4X1vaYYCj",
        "outputId": "6b9c552c-7a1c-4c10-d263-3f92db681662",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ayy4X1vaYYCj",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2589 12599\n",
            "LuckMetrics(dice_based=12, drawing_based=4, shuffling_based=2, random_based=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = '''you can only take this because it can be outrageous. \n",
        "#     you can't take it. you could not also choose. you may never be sure of the result. \n",
        "#     you can decide the next thing. he has no other choice but to stop, another option is winning.'''\n",
        "\n",
        "text = '''If player is unable to do this, then a subsequent player could place player house onto the fortification instead '''\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(clean_text(text))\n",
        "print([token.lemma_ for token in doc])\n",
        "print(len(doc), len(doc.text))\n",
        "\n",
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "I3JtzI3p9lvj",
        "outputId": "af4b9292-0004-4603-b6c2-e2b9812ffe34"
      },
      "id": "I3JtzI3p9lvj",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['if', 'player', 'be', 'unable', 'to', 'do', 'this', ',', 'then', 'a', 'subsequent', 'player', 'could', 'place', 'player', 'house', 'onto', 'the', 'fortification', 'instead']\n",
            "20 113\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9bd92d67699944cfa17f335ab38f14c2-0\" class=\"displacy\" width=\"3375\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">If</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">SCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">player</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">unable</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">do</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">this,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">then</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">subsequent</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">player</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">could</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">place</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">player</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">house</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">onto</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">fortification</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">instead</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,2.0 2150.0,2.0 2150.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-3\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M560.0,354.0 L568.0,342.0 552.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-5\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-6\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,89.5 2145.0,89.5 2145.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-8\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,177.0 1790.0,177.0 1790.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470,354.0 L1462,342.0 1478,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-9\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-10\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,177.0 2140.0,177.0 2140.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1820,354.0 L1812,342.0 1828,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-11\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,264.5 2135.0,264.5 2135.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1995,354.0 L1987,342.0 2003,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-12\" stroke-width=\"2px\" d=\"M2345,352.0 C2345,264.5 2485.0,264.5 2485.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2345,354.0 L2337,342.0 2353,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-13\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,177.0 2490.0,177.0 2490.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2490.0,354.0 L2498.0,342.0 2482.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-14\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,89.5 2670.0,89.5 2670.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2670.0,354.0 L2678.0,342.0 2662.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-15\" stroke-width=\"2px\" d=\"M2870,352.0 C2870,264.5 3010.0,264.5 3010.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2870,354.0 L2862,342.0 2878,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-16\" stroke-width=\"2px\" d=\"M2695,352.0 C2695,177.0 3015.0,177.0 3015.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3015.0,354.0 L3023.0,342.0 3007.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9bd92d67699944cfa17f335ab38f14c2-0-17\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,2.0 3200.0,2.0 3200.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9bd92d67699944cfa17f335ab38f14c2-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3200.0,354.0 L3208.0,342.0 3192.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "from collections import namedtuple\n",
        "from spacy.matcher import Matcher, DependencyMatcher\n",
        "\n",
        "def get_choices_amount_metric(doc: spacy.tokens.Doc) -> int:\n",
        "    # --------------  can/could/may/choose/select/... -------------- \n",
        "    # all can/could/may\n",
        "    can_could_may_matcher = Matcher(doc.vocab)\n",
        "    can_could_may_patterns = [\n",
        "        [{\n",
        "            \"LEMMA\": { \"IN\": [\"can\", \"could\", \"may\", \"decide\", \"select\", \"choose\", \"opt\"]}, \n",
        "            \"POS\": { \"IN\": [\"AUX\", \"VERB\"]}\n",
        "        }]\n",
        "    ]\n",
        "    can_could_may_matcher.add('can_could_may', can_could_may_patterns)\n",
        "    can_could_may_matches = { match[1] for match in can_could_may_matcher(doc) }\n",
        "\n",
        "    # can/could/may with only or neg\n",
        "    can_could_may_exceptions_matcher = DependencyMatcher(doc.vocab)\n",
        "    can_could_may_exceptions_patterns = [\n",
        "        [\n",
        "            # ❌ can not/only/never verb \n",
        "            {\n",
        "                \"RIGHT_ID\": \"can_could_may\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"can\", \"could\", \"may\"]}, \n",
        "                    \"POS\": \"AUX\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"can_could_may\",\n",
        "                \"REL_OP\": \"<\",\n",
        "                \"RIGHT_ID\": \"generic_verb\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"POS\": { \"IN\": [\"AUX\", \"VERB\"] }\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"generic_verb\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"neg_or_only\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"not\", \"only\", \"never\"]}, \n",
        "                    \"DEP\": { \"IN\": [\"advmod\", \"neg\"] }\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            # ❌ not/only/never choose\n",
        "            {\n",
        "                \"RIGHT_ID\": \"decision_verb\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"decide\", \"select\", \"choose\", \"opt\"]}, \n",
        "                    \"POS\": \"VERB\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"decision_verb\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"negation\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"not\", \"only\", \"never\"]}, \n",
        "                    \"DEP\": { \"IN\": [\"advmod\", \"neg\"] }\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            # ❌ can + choose are counted as 1. can token is left out\n",
        "            {\n",
        "                \"RIGHT_ID\": \"can_could_may\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"can\", \"could\", \"may\"]}, \n",
        "                    \"POS\": \"AUX\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"can_could_may\",\n",
        "                \"REL_OP\": \"<\",\n",
        "                \"RIGHT_ID\": \"decision_verb\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"decide\", \"select\", \"choose\", \"opt\"]},\n",
        "                    \"POS\": \"VERB\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "    can_could_may_exceptions_matcher.add('can_could_may_exceptions', can_could_may_exceptions_patterns)\n",
        "    can_could_may_exceptions_matches = { match[1][0] for match in can_could_may_exceptions_matcher(doc) }\n",
        "\n",
        "    # -------------- choice and option -------------- \n",
        "    choice_option_matcher = Matcher(doc.vocab)\n",
        "    choice_option_patterns = [\n",
        "        [{\n",
        "            \"LEMMA\": { \"IN\": [\"choice\", \"option\"]}, \n",
        "            \"POS\": \"NOUN\"\n",
        "        }]\n",
        "    ]\n",
        "    choice_option_matcher.add('choice_option', choice_option_patterns)\n",
        "    choice_option_matches = { match[1] for match in choice_option_matcher(doc) }\n",
        "\n",
        "    choice_option_exceptions_matcher = DependencyMatcher(doc.vocab)\n",
        "    choice_option_exceptions_patterns = [\n",
        "        [\n",
        "            {\n",
        "                \"RIGHT_ID\": \"choice\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": { \"IN\": [\"choice\", \"option\"]}, \n",
        "                    \"POS\": \"NOUN\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"LEFT_ID\": \"choice\",\n",
        "                \"REL_OP\": \">\",\n",
        "                \"RIGHT_ID\": \"prefix_no\",\n",
        "                \"RIGHT_ATTRS\": {\n",
        "                    \"LEMMA\": \"no\",\n",
        "                    \"POS\": \"DET\",\n",
        "                    \"DEP\": \"det\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    ]\n",
        "    choice_option_exceptions_matcher.add('choice_option_exceptions', choice_option_exceptions_patterns)\n",
        "    choice_option_exceptions_matches = { match[1][0] for match in choice_option_exceptions_matcher(doc) }\n",
        "\n",
        "    return len(can_could_may_matches.difference(can_could_may_exceptions_matches)) + \\\n",
        "           len(choice_option_matches.difference(choice_option_exceptions_matches))\n",
        "\n",
        "# text = get_document_by_line(CLEANED_DATASET_FILE_PATH, 130)\n",
        "text = '''you can only take this because it can be outrageous. \n",
        "    you can't take it. you can not also choose. you can never be sure of the result. \n",
        "    you can decide the next thing, or you choose the target. another choice is to win. \n",
        "    but there is no right option.'''\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(clean_text(text))\n",
        "print([(i, token.lemma_) for i, token in enumerate(doc)])\n",
        "print(len(doc), len(doc.text))\n",
        "print(get_choices_amount_metric(doc))\n",
        "\n",
        "# displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXvPodfk0ryj",
        "outputId": "faccb6a8-9347-440a-af8e-6a0ea1922209"
      },
      "id": "vXvPodfk0ryj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 'you'), (1, 'can'), (2, 'only'), (3, 'take'), (4, 'this'), (5, 'because'), (6, 'it'), (7, 'can'), (8, 'be'), (9, 'outrageous'), (10, '.'), (11, 'you'), (12, 'can'), (13, 'not'), (14, 'take'), (15, 'it'), (16, '.'), (17, 'you'), (18, 'can'), (19, 'not'), (20, 'also'), (21, 'choose'), (22, '.'), (23, 'you'), (24, 'can'), (25, 'never'), (26, 'be'), (27, 'sure'), (28, 'of'), (29, 'the'), (30, 'result'), (31, '.'), (32, 'you'), (33, 'can'), (34, 'decide'), (35, 'the'), (36, 'next'), (37, 'thing'), (38, ','), (39, 'or'), (40, 'you'), (41, 'choose'), (42, 'the'), (43, 'target'), (44, '.'), (45, 'another'), (46, 'choice'), (47, 'be'), (48, 'to'), (49, 'win'), (50, '.'), (51, 'but'), (52, 'there'), (53, 'be'), (54, 'no'), (55, 'right'), (56, 'option')]\n",
            "57 245\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(clean_text(get_document_by_line(CLEANED_DATASET_FILE_PATH, 155)))\n",
        "doc.text\n",
        "# print(doc[12].dep_)\n",
        "#doc.text.find('Tinners Trail Player')"
      ],
      "metadata": {
        "id": "XojL4wsdmTqI"
      },
      "id": "XojL4wsdmTqI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multi_rake import Rake\n",
        "from summa import keywords\n",
        "import yake\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = clean_text(get_document_by_line(CLEANED_DATASET_FILE_PATH, 155))\n",
        "\n",
        "def use_rake(text: str):\n",
        "    rake = Rake()\n",
        "    keywords = rake.apply(text)\n",
        "    return keywords[:30]\n",
        "\n",
        "def use_yake(text: str):\n",
        "    kw_extractor = yake.KeywordExtractor(top=20)\n",
        "    keywords_info = kw_extractor.extract_keywords(text)\n",
        "    keyword_groups = [keyword_info[0] for keyword_info in keywords_info if keyword_info[1] < 0.1]\n",
        "    return keyword_groups\n",
        "\n",
        "def use_TextRank(text: str):\n",
        "    TR_keywords = keywords.keywords(text, scores=True)\n",
        "    return TR_keywords\n",
        "\n",
        "display(use_rake(text))\n",
        "display(use_TextRank(text))\n",
        "use_yake(text)"
      ],
      "metadata": {
        "id": "MAU5oGis58L5"
      },
      "id": "MAU5oGis58L5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Set, Dict\n",
        "from rake_nltk import Rake\n",
        "from nltk.util import ngrams\n",
        "import yake\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "\n",
        "regex_word_within_boundaries = re.compile(r'\\b')\n",
        "MIN_TOKEN_TO_BE_CONSIDERED_COMPONENT = 4\n",
        "MAX_COMPONENTS = 100\n",
        "\n",
        "def find_most_common_nouns(doc: spacy.tokens.Doc) -> Dict[str, List[spacy.tokens.Token]]:\n",
        "    tokens_dict = defaultdict(list)\n",
        "\n",
        "    for token in doc:\n",
        "        if len(token) >= 3 and \\\n",
        "            token.pos_ in ['NOUN', 'PROPN'] and \\\n",
        "            token.dep_ in ['nsubj', 'dobj', 'nsubjpass', 'pobj', 'compound']:\n",
        "            tokens_dict[token.lemma_.lower()].append(token)\n",
        "           \n",
        "    return tokens_dict\n",
        "\n",
        "def _get_ngrams_components(doc: spacy.tokens.Doc, \\\n",
        "                           components: Dict[str, List[spacy.tokens.Token]],\n",
        "                           n_grams: int):\n",
        "    pass\n",
        "\n",
        "\n",
        "def _get_bg_components_by_deps_inspection(doc: spacy.tokens.Doc) -> Dict[str, List[int]]:\n",
        "    words_to_leave_out = ['beginning', 'board', 'book', 'case', 'clarification', 'design', \n",
        "                          'effect', 'end', 'example', 'case', 'game', 'number', \n",
        "                          'overview', 'order', 'play', 'player', 'purpose', 'reference',\n",
        "                          'result', 'rule', 'rulebook', 'section', 'set', 'setup', 'side', 'summary', \n",
        "                          'start', 'step', 'thing', 'type', 'time', 'total', 'use', 'value', 'version', 'way']\n",
        "\n",
        "    possible_components_info = dict(filter(lambda token: token[0] not in words_to_leave_out and \n",
        "                                           len(token[1]) >= MIN_TOKEN_TO_BE_CONSIDERED_COMPONENT and\n",
        "                                           any(token_occurrence.dep_ in ['nsubj', 'nsubjpass'] \n",
        "                                               for token_occurrence in token[1]), \n",
        "                                           find_most_common_nouns(doc).items()))\n",
        "    return possible_components_info.keys()\n",
        "\n",
        "# def _get_lemmas_given_keywords_group(group: str, doc: spacy.tokens.Doc) -> List[str]:\n",
        "#     kw_match = re.search(r'\\b' + group + '\\\\b', doc.text)\n",
        "#     if kw_match is None:\n",
        "#         return []\n",
        "\n",
        "#     group_span = doc.char_span(kw_match.start(0), kw_match.end(0))\n",
        "#     return [token.lemma_.lower() for token in group_span]\n",
        "\n",
        "# def _get_bg_components_by_keyword_analysis(doc: spacy.tokens.Doc, max_keywords: int) -> List[str]:\n",
        "#     kw_extractor = yake.KeywordExtractor(top=max_keywords)\n",
        "#     keywords_info = kw_extractor.extract_keywords(doc.text)\n",
        "#     keyword_groups = [keyword_info[0] for keyword_info in keywords_info if keyword_info[1] < 0.1]\n",
        "\n",
        "#     return [lemma for keyword_group in keyword_groups \n",
        "#             for lemma in _get_lemmas_given_keywords_group(keyword_group, doc)]\n",
        "\n",
        "def get_bg_components(doc: spacy.tokens.Doc) -> Dict[str, List[int]]:\n",
        "    components_by_deps = _get_bg_components_by_deps_inspection(doc)\n",
        "    # print(components_by_deps)\n",
        "    # components_by_kws = _get_bg_components_by_keyword_analysis(doc, len(components_by_deps))\n",
        "    # print(components_by_kws)\n",
        "\n",
        "    # return set(components_by_deps).intersection(set(components_by_kws))\n",
        "    return components_by_deps\n",
        "\n",
        "def get_doc_variance(doc: spacy.tokens.Doc, components_dict: Dict[str, List[int]]) -> float:\n",
        "    '''variance measures how components interleave in the text. This could mean that rules involve\n",
        "    many components and are therefore more complex. variancy is computed using `np.var` on each\n",
        "    component list. the results are normalized by multiplicating for the frequency of the component.\n",
        "    eventually the partial variances are summed together and the result normalized with the \n",
        "    total numbers of tokens.'''\n",
        "    tokens_count = sum(len(token_list) for token_list in components_dict.values())\n",
        "    return sum((len(tokens) / tokens_count) * np.var([token.i for token in tokens])\n",
        "        for tokens in components_dict.values()) / len((doc))\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(clean_text(get_document_by_line(CLEANED_DATASET_FILE_PATH, 103)))\n",
        "display(doc.text)\n",
        "components = get_bg_components(doc)\n",
        "display(components)\n",
        "# print(get_doc_variance(doc, components))\n",
        "# dict_keys(['scoring', 'marker', 'emperor', 'color', 'region', 'card', 'house', \n",
        "#            'emissary', 'space', 'fortification', 'hand', 'draw', 'pile', 'point', \n",
        "#            'place', 'road', 'dragon', 'alliance', 'alex', 'barbara', 'joker', \n",
        "#            'turn', 'piece', 'majority', 'wei', 'chris', 'doris', 'tie', 'row'])\n",
        "# dict_keys(['scoring', 'marker', 'card', 'house', 'emissary', 'space', 'fortification', \n",
        "#            'pile', 'point', 'place', 'alliance', 'alex', 'barbara', 'turn', \n",
        "#            'piece', 'majority', 'chris', 'doris', 'row'])"
      ],
      "metadata": {
        "id": "TKh5Jf-xovbn"
      },
      "id": "TKh5Jf-xovbn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "\n",
        "def find_n_most_common_nouns(n, docs: List[spacy.tokens.Doc]) -> List[str]:\n",
        "    docs_sets = [set(find_most_common_nouns(doc).keys())\n",
        "                 for doc in docs]\n",
        "    all_tokens_from_docs = itertools.chain(*docs_sets)\n",
        "    tokens_counter = Counter(all_tokens_from_docs)\n",
        "    return tokens_counter.most_common(n)\n",
        "    \n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "df_dataset = get_df_with_docs(CLEANED_DATASET_FILE_PATH, 10, 50)\n",
        "docs = nlp.pipe(map(clean_text, df_dataset['rulebook'].values))\n",
        "\n",
        "find_n_most_common_nouns(10, docs)"
      ],
      "metadata": {
        "id": "w-JVFrfg4BwA",
        "outputId": "6c3dfc8e-f4f4-4151-c7d4-03fb8b66178a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "w-JVFrfg4BwA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('game', 10),\n",
              " ('turn', 10),\n",
              " ('player', 10),\n",
              " ('point', 9),\n",
              " ('number', 9),\n",
              " ('play', 9),\n",
              " ('hand', 8),\n",
              " ('end', 8),\n",
              " ('side', 8),\n",
              " ('way', 8)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d6596cd-3fd1-4e50-9dce-e6c333667182",
      "metadata": {
        "id": "7d6596cd-3fd1-4e50-9dce-e6c333667182"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import pandas as pd\n",
        "import ast\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def get_rules_features(id: int, doc: spacy.tokens.Doc) -> Tuple[int, float]:\n",
        "    logger.info(f'processing board game {id}')\n",
        "    rulebook_len = len(doc)\n",
        "    bg_components = get_bg_components(doc)\n",
        "    print(bg_components)\n",
        "\n",
        "    return 0, 0\n",
        "    # rules = get_rules(text)\n",
        "    # rule_count = len(rules)\n",
        "    # return rule_count, len(text) / rule_count\n",
        "\n",
        "def apply_for_rulebook_features(row, docs_dict):\n",
        "    next_doc_info = next(docs_dict)\n",
        "    assert next_doc_info[0] == row.id\n",
        "    return pd.Series(get_rules_features(row.id, next_doc_info[1]), \n",
        "                     index=['rule_count', 'avg_rule_len'])\n",
        "\n",
        "PROCESSED_DATASET_FILE_PATH = 'data/processed_dataset.csv' if WORKING_LOCALLY \\\n",
        "    else '/content/drive/My Drive/Projects/IRBoardGameComplexity/processed_dataset.csv'\n",
        "\n",
        "# ast.literal_eval converts the family column string into a python array\n",
        "df_dataset = pd.read_csv(CLEANED_DATASET_FILE_PATH, converters={ 'family': ast.literal_eval }, nrows=1)\n",
        "remove_columns_prefix(df_dataset)\n",
        "docs_dict = zip(df_dataset['id'].values, \n",
        "                nlp.pipe(map(clean_text, df_dataset['rulebook'].values)))\n",
        "\n",
        "df_rules_features = df_dataset.apply(lambda x: apply_for_rulebook_features(x, docs_dict),\n",
        "                                     axis='columns')\n",
        "df_features = df_dataset[['averageweight', 'playingtime', 'family']].join(df_rules_features)\n",
        "        \n",
        "# one-hot encoding \"family\" field \n",
        "# from https://stackoverflow.com/questions/71401193/one-hot-encoding-in-python-for-array-values-in-a-dataframe\n",
        "df_features = df_features.join(df_features.pop('family').apply('|'.join).str.get_dummies())\n",
        "df_features.head()\n",
        "\n",
        "# df_features.to_csv(PROCESSED_DATASET_FILE_PATH, header=True, index=False, mode='w')    \n",
        "# if not WORKING_LOCALLY:\n",
        "#     drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}