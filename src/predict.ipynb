{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa4a086-aa20-4def-b17a-3b4ff4fad93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2022 19:13:20 - INFO - \t missing_keys: []\n",
      "11/07/2022 19:13:20 - INFO - \t unexpected_keys: []\n",
      "11/07/2022 19:13:20 - INFO - \t mismatched_keys: []\n",
      "11/07/2022 19:13:20 - INFO - \t error_msgs: []\n",
      "11/07/2022 19:13:20 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fastcoref.spacy_component.spacy_component.FastCorefResolver at 0x7f39189af8e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastcoref import spacy_component\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\", \"lemmatizer\", \"ner\", \"textcat\"])\n",
    "nlp.add_pipe(\"fastcoref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4bcea26-c1a3-45fb-845e-98cae3073e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 19:13:43,597 bgg_predict  DEBUG    test\n",
      "11/07/2022 19:13:43 - DEBUG - \t test\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger('bgg_predict')\n",
    "logger.handlers.clear()\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        '%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "logger.debug('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4caa1a08-119d-4cca-a7fc-61808c35e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "regex_mail = re.compile(r'\\w+(?:\\.\\w+)*?@\\w+(?:\\.\\w+)+')\n",
    "\n",
    "@dataclass\n",
    "class Sentence:\n",
    "    content: str\n",
    "    start: int\n",
    "    end: int\n",
    "    \n",
    "    def does_include_pos(self, pos: int) -> bool:\n",
    "        return self.start <= pos <= self.end\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    return regex_mail.sub('', text)\n",
    "\n",
    "def get_sentences_from_text(text: str) -> List[Sentence]:\n",
    "    # assert no continuous dots because of text cleared while building the dataset\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    res = []\n",
    "    char_accumulator = 0\n",
    "    for sentence in sentences:\n",
    "        res.append(Sentence(sentence, char_accumulator, char_accumulator + len(sentence) - 1))\n",
    "        char_accumulator += len(sentence) + 1\n",
    "        \n",
    "    if res[-1].content == '':\n",
    "        res.pop()\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "361b1290-00e4-42f3-85a4-badadb8d574f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 1]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bisect import bisect_left\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# necessary to use bisect_left with ranges\n",
    "@dataclass\n",
    "class Interval:\n",
    "    start: int\n",
    "    end: int\n",
    "    \n",
    "    def __lt__(self, other) -> bool:\n",
    "       return self.start < self.end < other.start\n",
    "    \n",
    "    def __eq__(self, other) -> bool:\n",
    "       return self.start <= other.start <= self.end\n",
    "\n",
    "def get_sentences_from_clusters(clusters: List[List[Tuple[int, int]]], sentences: List[Sentence]) -> List[List[int]]:\n",
    "    '''find the sentence each cluster belongs to'''\n",
    "    sentence_clusters = []\n",
    "    for cluster in clusters:\n",
    "        sentence_clusters.append([bisect_left(sentences, Interval(entity[0], entity[1]), key=lambda x: Interval(x.start, x.end)) for entity in cluster])\n",
    "\n",
    "    return sentence_clusters\n",
    "\n",
    "text = 'Alice goes down the rabbit hole. Where she would discover a new reality beyond her expectations.'\n",
    "sentences = get_sentences_from_text(text)\n",
    "clusters = [[(0, 5), (39, 42), (79, 82)]]\n",
    "get_sentences_from_clusters(clusters, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e53dc33-31ba-4e04-80e6-dc645714fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "def get_rule_groups_from_sentence_clusters(sentences: List[Sentence], sentence_clusters: List[List[int]]) -> List[List[int]]:\n",
    "    def normalize_group(group: Set[int]) -> List[List[int]]:\n",
    "        '''each group could contain multiple consecutive sublists. this method split these sublists'''\n",
    "        res = []\n",
    "\n",
    "        # https://stackoverflow.com/a/23861347/5587393\n",
    "        for k, g in groupby(enumerate(sorted(list(group))), lambda x: x[0] - x[1]):\n",
    "            res.append(list(map(itemgetter(1), g)))\n",
    "\n",
    "        return res\n",
    "    # the graph is built as a directed sparse graph where the first element of each cluster\n",
    "    # is connected to the other elements in the same cluster\n",
    "    graph = [[0 for _ in range(len(sentences))] for __ in range(len(sentences))]\n",
    "    for cluster in sentence_clusters:\n",
    "        for sentence in cluster[1:]:\n",
    "            graph[cluster[0]][sentence] = 1\n",
    "\n",
    "    # find the connected components of the graph created from the clusters returned after coref     \n",
    "    graph = csr_matrix(graph)\n",
    "    n_components, labels = connected_components(csgraph=graph, directed=False, return_labels=True)\n",
    "    groups = [set() for _ in range(n_components)]\n",
    "    for i, label in enumerate(labels):\n",
    "        groups[label].add(i)\n",
    "\n",
    "    return [norm_group for group in groups for norm_group in normalize_group(group)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6596cd-3fd1-4e50-9dce-e6c333667182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2022 19:14:03 - INFO - \t Tokenize 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890aefb4be9f48f1a2b6bc02ef6c2765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2022 19:14:05 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940b89899fed471b82141477b7997cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 19:14:07,924 bgg_predict  DEBUG    [[(200, 211), (287, 296), (929, 938), (1520, 1529)], [(439, 453), (463, 467), (530, 546)], [(632, 648), (649, 651)], [(714, 741), (754, 756)], [(689, 700), (777, 781), (840, 851)], [(785, 799), (819, 821)], [(917, 938), (986, 998)], [(1217, 1228), (1235, 1238), (1267, 1269)], [(392, 406), (1295, 1311)], [(1484, 1492), (1499, 1502), (1533, 1535)], [(468, 489), (1975, 1996)], [(2027, 2033), (2069, 2082), (2108, 2110)], [(2219, 2227), (2237, 2245), (2569, 2577), (2681, 2689), (2708, 2716)], [(2278, 2286), (2306, 2309), (2350, 2391)], [(2406, 2451), (2443, 2446)], [(2511, 2522), (2591, 2603), (2645, 2649)], [(2581, 2590), (2664, 2666)], [(2336, 2349), (2723, 2736)], [(2999, 3011), (3100, 3112)], [(2912, 2921), (3123, 3132)]]\n",
      "11/07/2022 19:14:07 - DEBUG - \t [[(200, 211), (287, 296), (929, 938), (1520, 1529)], [(439, 453), (463, 467), (530, 546)], [(632, 648), (649, 651)], [(714, 741), (754, 756)], [(689, 700), (777, 781), (840, 851)], [(785, 799), (819, 821)], [(917, 938), (986, 998)], [(1217, 1228), (1235, 1238), (1267, 1269)], [(392, 406), (1295, 1311)], [(1484, 1492), (1499, 1502), (1533, 1535)], [(468, 489), (1975, 1996)], [(2027, 2033), (2069, 2082), (2108, 2110)], [(2219, 2227), (2237, 2245), (2569, 2577), (2681, 2689), (2708, 2716)], [(2278, 2286), (2306, 2309), (2350, 2391)], [(2406, 2451), (2443, 2446)], [(2511, 2522), (2591, 2603), (2645, 2649)], [(2581, 2590), (2664, 2666)], [(2336, 2349), (2723, 2736)], [(2999, 3011), (3100, 3112)], [(2912, 2921), (3123, 3132)]]\n",
      "11/07/2022 19:14:09 - INFO - \t Tokenize 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc8d4cfcd744ce092f17b0915acbddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2022 19:14:11 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8dd22ede6f4dfd9908b711b58a1e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_FILE_PATH = 'data/dataset.csv'\n",
    "\n",
    "def get_rules(text: str) -> List[str]:\n",
    "    text = clean_text(text)\n",
    "    sentences = get_sentences_from_text(text)\n",
    "    \n",
    "    doc = nlp(text, component_cfg = { \"fastcoref\": {'resolve_text': True} })\n",
    "    coref_clusters = doc._.coref_clusters\n",
    "    logger.debug(coref_clusters)\n",
    "    \n",
    "    sentence_clusters = get_sentences_from_clusters(coref_clusters, sentences)\n",
    "    rule_groups = get_rule_groups_from_sentence_clusters(sentences, sentence_clusters)\n",
    "    \n",
    "    return ['. '.join([sentences[s_index].content for s_index in group]) for group in rule_groups]\n",
    "\n",
    "def get_rules_features(text: str) -> Tuple[int, float]:\n",
    "    rules = get_rules(text)\n",
    "    rule_count = len(rules)\n",
    "    return rule_count, len(text) / rule_count\n",
    "\n",
    "def remove_columns_prefix(df: pd.core.frame.DataFrame) -> None:\n",
    "    '''remove prefix 'info.' from the columns of df'''\n",
    "    df.rename(columns=lambda c: c.rsplit('.', 1)[-1], inplace=True)\n",
    "    \n",
    "df_features = pd.DataFrame()\n",
    "with pd.read_csv(DATASET_FILE_PATH, chunksize=1) as reader:\n",
    "    for df in reader:\n",
    "        remove_columns_prefix(df)\n",
    "        df_rules_features = df.apply(lambda x: pd.Series(get_rules_features(x.rulebook), \n",
    "                                     index=['rule_count', 'avg_rule_len']), axis='columns')\n",
    "        df_features = pd.concat([df_features, df[['numweights', 'averageweight', 'playingtime', 'family']].join(df_rules_features)])\n",
    "        \n",
    "display(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642bfade-a7c0-48cc-9191-8a681c36241b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
